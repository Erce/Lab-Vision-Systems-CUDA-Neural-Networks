{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "focused-magic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import optuna\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "experienced-chemical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data\\train_32x32.mat\n",
      "Using downloaded and verified file: ./data\\test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "trainset = datasets.SVHN(root='./data', split='train', transform=transforms.ToTensor(),download=True)\n",
    "testset = datasets.SVHN(root='./data', split='test', transform=transforms.ToTensor(),download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dangerous-architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "recovered-marsh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "likely-marathon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn model\n",
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Model initializer \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # layer 1\n",
    "        conv1 = nn.Conv2d(in_channels=3, out_channels=32,  kernel_size=3, stride=1, padding=0)\n",
    "        relu1 = nn.ReLU()\n",
    "        maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.layer1 = nn.Sequential(\n",
    "                conv1, relu1, maxpool1\n",
    "            )\n",
    "      \n",
    "        # layer 2\n",
    "        conv2 = nn.Conv2d(in_channels= 32, out_channels=64,  kernel_size=3, stride=1, padding=0)\n",
    "        relu2 = nn.ReLU()\n",
    "        maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.layer2 = nn.Sequential(\n",
    "                conv2, relu2, maxpool2\n",
    "            )\n",
    "        \n",
    "        # layer 3\n",
    "        conv3 = nn.Conv2d(in_channels=64, out_channels=128,  kernel_size=3, stride=1, padding=0)\n",
    "        relu3 = nn.ReLU()\n",
    "        maxpool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.layer3 = nn.Sequential(\n",
    "                conv3, relu3, maxpool3\n",
    "            )\n",
    "                \n",
    "        # fully connected classifier\n",
    "        in_dim = 128 * 2 * 2\n",
    "        self.fc = nn.Linear(in_features=in_dim, out_features=10)\n",
    "\n",
    "        return\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass \"\"\"\n",
    "        cur_b_size = x.shape[0]\n",
    "        out1 = self.layer1(x)\n",
    "        out2 = self.layer2(out1)\n",
    "        out3 = self.layer3(out2)\n",
    "        out3_flat = out3.view(cur_b_size, -1)\n",
    "        y = self.fc(out3_flat)\n",
    "        return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "former-occurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Trainer:\n",
    "    \n",
    "    def __init__(self, model, criterion, optimizer, save_freq, batch_size, learning_rate):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.save_freq = save_freq\n",
    "        self.stats = {\n",
    "        \"epoch\": [],\n",
    "        \"train_loss\": [],\n",
    "        }\n",
    "        self.train_loader = torch.utils.data.DataLoader(trainset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "        self.test_loader=torch.utils.data.DataLoader(trainset,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=True)\n",
    "        \n",
    "        \n",
    "    def train(self, n_epochs, init_epoch=0):\n",
    "        loss_hist  = []\n",
    "        for epoch in range(init_epoch, n_epochs):\n",
    "            loss_list = []\n",
    "            progress_bar = tqdm(enumerate(self.train_loader), total=len(self.train_loader))\n",
    "            for i, (images, labels) in progress_bar:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "        \n",
    "                # Clear gradients w.r.t. parameters\n",
    "                self.optimizer.zero_grad()\n",
    "         \n",
    "                # Forward pass to get output/logits\n",
    "                outputs = self.model(images)\n",
    "         \n",
    "                # Calculate Loss: softmax --> cross entropy loss\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss_list.append(loss.item())\n",
    "         \n",
    "                # Getting gradients w.r.t. parameters\n",
    "                loss.backward()\n",
    "                     \n",
    "                # Updating parameters\n",
    "                self.optimizer.step()\n",
    "                progress_bar.set_description(f\"Epoch {epoch+1} Iter {i+1}: loss {loss.item():.5f}. \")\n",
    "             \n",
    "        loss_hist.append(np.mean(loss_list))\n",
    "        self.stats[\"epoch\"].append(epoch)\n",
    "        self.stats[\"train_loss\"].append(loss_hist[-1])\n",
    "    \n",
    "        ## saving checkpoint\n",
    "        #if epoch % self.save_freq == 0:\n",
    "        #    save_model(model=cnn, optimizer=optimizer, epoch=epoch, stats=stats)\n",
    "        return loss_hist\n",
    "    \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def eval_model(self):\n",
    "        \"\"\" Computing model accuracy \"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loss_list = []\n",
    "    \n",
    "        for images, labels in self.test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "        \n",
    "            # Forward pass only to get logits/output\n",
    "            outputs = self.model(images)\n",
    "                 \n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss_list.append(loss.item())\n",
    "            \n",
    "            # Get predictions from the maximum value\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += len( torch.where(preds==labels)[0] )\n",
    "            total += len(labels)\n",
    "                 \n",
    "        # Total correct predictions and loss\n",
    "        accuracy = correct / total * 100\n",
    "        loss = np.mean(loss_list)\n",
    "        return accuracy, loss\n",
    "\n",
    "\n",
    "    def save_model(self, optimizer, epoch, stats):\n",
    "        \"\"\" Saving model checkpoint \"\"\"\n",
    "    \n",
    "        if(not os.path.exists(\"models\")):\n",
    "            os.makedirs(\"models\")\n",
    "        savepath = f\"models/checkpoint_epoch_{epoch}.pth\"\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'stats': stats\n",
    "        }, savepath)\n",
    "        return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "affiliated-journal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining model\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim=3072, hidden_dim=256, output_dim=10):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.model = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, output_dim)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_flat = x.view(-1, self.input_dim)\n",
    "        out = self.model(x_flat)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "professional-technology",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR_Trainer:\n",
    "    def __init__(self, model, criterion, optimizer, save_freq, batch_size, learning_rate):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.save_freq = save_freq\n",
    "        self.stats = {\n",
    "        \"epoch\": [],\n",
    "        \"train_loss\": [],\n",
    "        }\n",
    "        self.train_loader = torch.utils.data.DataLoader(trainset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "        self.test_loader=torch.utils.data.DataLoader(trainset,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=True)\n",
    "        \n",
    "    def train(self, n_epochs, init_epoch=0 ):\n",
    "        loss_hist  = []\n",
    "        for epoch in range(n_epochs):\n",
    "            loss_list = []\n",
    "            progress_bar = tqdm(enumerate(self.train_loader), total=len(self.train_loader))\n",
    "            for i, (imgs, labels) in progress_bar:\n",
    "        \n",
    "                #everything needs to be on the same device\n",
    "                imgs = imgs.to(device)\n",
    "                labels = labels.to(device)\n",
    "        \n",
    "                # forward pass\n",
    "                pred_labels = self.model(imgs)\n",
    "    \n",
    "                # computing error\n",
    "                loss = criterion(pred_labels, labels)\n",
    "                loss_list.append(loss.item())\n",
    "\n",
    "                # removing accumulated gradients\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                    \n",
    "                if(i % 1 == 0 or i == N_ITERS-1):\n",
    "                    progress_bar.set_description(f\"Epoch {epoch+1} Iter {i+1}: loss {loss.item():.5f}. \")\n",
    "            \n",
    "        loss_hist.append(np.mean(loss_list))\n",
    "        self.stats[\"epoch\"].append(epoch)\n",
    "        self.stats[\"train_loss\"].append(loss_hist[-1])  \n",
    "        \n",
    "        return loss_hist\n",
    "\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def eval_model(self):\n",
    "        \"\"\" Computing model accuracy \"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loss_list = []\n",
    "    \n",
    "        for images, labels in self.test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "        \n",
    "            # Forward pass only to get logits/output\n",
    "            outputs = self.model(images)\n",
    "                 \n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss_list.append(loss.item())\n",
    "            \n",
    "            # Get predictions from the maximum value\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += len( torch.where(preds==labels)[0] )\n",
    "            total += len(labels)\n",
    "                 \n",
    "        # Total correct predictions and loss\n",
    "        accuracy = correct / total * 100\n",
    "        loss = np.mean(loss_list)\n",
    "        return accuracy, loss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mobile-thomas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizing learning rate and batch size for cnn\n",
    "def train_with_optimizer(N_EPOCHS, model, structure, criterion, learning_rate=3e-3, batch_size=256):\n",
    "    \n",
    "    print('Learning rate:', learning_rate, 'Batch size:', batch_size)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    if structure == \"cnn\":\n",
    "        trainer = CNN_Trainer(model, criterion, optimizer,1000, batch_size, learning_rate)\n",
    "    elif structure == \"lr\":\n",
    "        trainer = LR_Trainer(model, criterion, optimizer,1000, batch_size, learning_rate)\n",
    "    \n",
    "    trainer.train(N_EPOCHS)\n",
    "    accuracy, _ = trainer.eval_model()\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "worse-indication",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-06-08 14:52:15,478]\u001b[0m A new study created in memory with name: no-name-1ae205fb-f631-4d62-942b-2ce74a944614\u001b[0m\n",
      "  0%|                                                                                          | 0/187 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.001793528644326713 Batch size: 392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iter 187: loss 0.93666. : 100%|██████████████████████████████████████████████| 187/187 [00:18<00:00, 10.02it/s]\n",
      "Epoch 2 Iter 187: loss 0.63156. : 100%|██████████████████████████████████████████████| 187/187 [00:11<00:00, 16.81it/s]\n",
      "Epoch 3 Iter 187: loss 0.59719. : 100%|██████████████████████████████████████████████| 187/187 [00:11<00:00, 16.87it/s]\n",
      "Epoch 4 Iter 187: loss 0.60935. : 100%|██████████████████████████████████████████████| 187/187 [00:11<00:00, 16.97it/s]\n",
      "Epoch 5 Iter 187: loss 0.54279. : 100%|██████████████████████████████████████████████| 187/187 [00:10<00:00, 17.05it/s]\n",
      "Epoch 6 Iter 187: loss 0.37106. : 100%|██████████████████████████████████████████████| 187/187 [00:11<00:00, 16.98it/s]\n",
      "Epoch 7 Iter 187: loss 0.43322. : 100%|██████████████████████████████████████████████| 187/187 [00:11<00:00, 16.97it/s]\n",
      "Epoch 8 Iter 187: loss 0.35698. : 100%|██████████████████████████████████████████████| 187/187 [00:10<00:00, 17.46it/s]\n",
      "Epoch 9 Iter 187: loss 0.37789. : 100%|██████████████████████████████████████████████| 187/187 [00:10<00:00, 17.53it/s]\n",
      "Epoch 10 Iter 187: loss 0.31140. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.43it/s]\n",
      "Epoch 11 Iter 187: loss 0.29483. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.50it/s]\n",
      "Epoch 12 Iter 187: loss 0.27874. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.46it/s]\n",
      "Epoch 13 Iter 187: loss 0.28909. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.41it/s]\n",
      "Epoch 14 Iter 187: loss 0.25404. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.40it/s]\n",
      "Epoch 15 Iter 187: loss 0.28994. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.35it/s]\n",
      "Epoch 16 Iter 187: loss 0.29061. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.22it/s]\n",
      "Epoch 17 Iter 187: loss 0.30266. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.40it/s]\n",
      "Epoch 18 Iter 187: loss 0.24306. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.43it/s]\n",
      "Epoch 19 Iter 187: loss 0.20973. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.46it/s]\n",
      "Epoch 20 Iter 187: loss 0.30281. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.39it/s]\n",
      "Epoch 21 Iter 187: loss 0.26192. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.49it/s]\n",
      "Epoch 22 Iter 187: loss 0.24614. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.50it/s]\n",
      "Epoch 23 Iter 187: loss 0.16070. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.48it/s]\n",
      "Epoch 24 Iter 187: loss 0.26646. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.40it/s]\n",
      "Epoch 25 Iter 187: loss 0.23071. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.39it/s]\n",
      "Epoch 26 Iter 187: loss 0.16466. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.32it/s]\n",
      "Epoch 27 Iter 187: loss 0.15357. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.31it/s]\n",
      "Epoch 28 Iter 187: loss 0.26639. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.43it/s]\n",
      "Epoch 29 Iter 187: loss 0.29116. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.38it/s]\n",
      "Epoch 30 Iter 187: loss 0.16822. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.25it/s]\n",
      "Epoch 31 Iter 187: loss 0.16740. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.34it/s]\n",
      "Epoch 32 Iter 187: loss 0.29066. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.39it/s]\n",
      "Epoch 33 Iter 187: loss 0.23524. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.37it/s]\n",
      "Epoch 34 Iter 187: loss 0.15180. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.45it/s]\n",
      "Epoch 35 Iter 187: loss 0.19305. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.18it/s]\n",
      "Epoch 36 Iter 187: loss 0.13858. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.33it/s]\n",
      "Epoch 37 Iter 187: loss 0.21741. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.39it/s]\n",
      "Epoch 38 Iter 187: loss 0.15692. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.47it/s]\n",
      "Epoch 39 Iter 187: loss 0.15593. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.44it/s]\n",
      "Epoch 40 Iter 187: loss 0.13842. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.38it/s]\n",
      "Epoch 41 Iter 187: loss 0.12166. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.26it/s]\n",
      "Epoch 42 Iter 187: loss 0.20042. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.44it/s]\n",
      "Epoch 43 Iter 187: loss 0.24897. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.32it/s]\n",
      "Epoch 44 Iter 187: loss 0.16124. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.42it/s]\n",
      "Epoch 45 Iter 187: loss 0.13552. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.39it/s]\n",
      "Epoch 46 Iter 187: loss 0.19955. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.07it/s]\n",
      "Epoch 47 Iter 187: loss 0.11977. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.32it/s]\n",
      "Epoch 48 Iter 187: loss 0.11465. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.34it/s]\n",
      "Epoch 49 Iter 187: loss 0.13477. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.44it/s]\n",
      "Epoch 50 Iter 187: loss 0.13478. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.34it/s]\n",
      "Epoch 51 Iter 187: loss 0.12943. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.43it/s]\n",
      "Epoch 52 Iter 187: loss 0.10150. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.45it/s]\n",
      "Epoch 53 Iter 187: loss 0.13639. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.37it/s]\n",
      "Epoch 54 Iter 187: loss 0.12373. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.39it/s]\n",
      "Epoch 55 Iter 187: loss 0.10818. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.44it/s]\n",
      "Epoch 56 Iter 187: loss 0.14162. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.37it/s]\n",
      "Epoch 57 Iter 187: loss 0.10272. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.40it/s]\n",
      "Epoch 58 Iter 187: loss 0.07768. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.26it/s]\n",
      "Epoch 59 Iter 187: loss 0.09486. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.42it/s]\n",
      "Epoch 60 Iter 187: loss 0.09722. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.29it/s]\n",
      "Epoch 61 Iter 187: loss 0.11385. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.40it/s]\n",
      "Epoch 62 Iter 187: loss 0.14185. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.44it/s]\n",
      "Epoch 63 Iter 187: loss 0.12470. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.25it/s]\n",
      "Epoch 64 Iter 187: loss 0.07602. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.45it/s]\n",
      "Epoch 65 Iter 187: loss 0.08751. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.45it/s]\n",
      "Epoch 66 Iter 187: loss 0.14912. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.34it/s]\n",
      "Epoch 67 Iter 187: loss 0.08901. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.46it/s]\n",
      "Epoch 68 Iter 187: loss 0.13074. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.33it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69 Iter 187: loss 0.12025. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.37it/s]\n",
      "Epoch 70 Iter 187: loss 0.14763. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.41it/s]\n",
      "Epoch 71 Iter 187: loss 0.11346. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.36it/s]\n",
      "Epoch 72 Iter 187: loss 0.10307. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.37it/s]\n",
      "Epoch 73 Iter 187: loss 0.07111. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.31it/s]\n",
      "Epoch 74 Iter 187: loss 0.14134. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.38it/s]\n",
      "Epoch 75 Iter 187: loss 0.13762. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.41it/s]\n",
      "Epoch 76 Iter 187: loss 0.09830. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.31it/s]\n",
      "Epoch 77 Iter 187: loss 0.07696. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.44it/s]\n",
      "Epoch 78 Iter 187: loss 0.07456. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.40it/s]\n",
      "Epoch 79 Iter 187: loss 0.11547. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.38it/s]\n",
      "Epoch 80 Iter 187: loss 0.14219. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.45it/s]\n",
      "Epoch 81 Iter 187: loss 0.07922. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.13it/s]\n",
      "Epoch 82 Iter 187: loss 0.03713. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.33it/s]\n",
      "Epoch 83 Iter 187: loss 0.08271. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.31it/s]\n",
      "Epoch 84 Iter 187: loss 0.04715. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.23it/s]\n",
      "Epoch 85 Iter 187: loss 0.07109. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.58it/s]\n",
      "Epoch 86 Iter 187: loss 0.05705. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.44it/s]\n",
      "Epoch 87 Iter 187: loss 0.09850. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.47it/s]\n",
      "Epoch 88 Iter 187: loss 0.04105. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.26it/s]\n",
      "Epoch 89 Iter 187: loss 0.04808. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.46it/s]\n",
      "Epoch 90 Iter 187: loss 0.04374. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.52it/s]\n",
      "Epoch 91 Iter 187: loss 0.11270. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.48it/s]\n",
      "Epoch 92 Iter 187: loss 0.11325. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.42it/s]\n",
      "Epoch 93 Iter 187: loss 0.06908. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.44it/s]\n",
      "Epoch 94 Iter 187: loss 0.02641. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.49it/s]\n",
      "Epoch 95 Iter 187: loss 0.04767. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.49it/s]\n",
      "Epoch 96 Iter 187: loss 0.07757. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.42it/s]\n",
      "Epoch 97 Iter 187: loss 0.04871. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.33it/s]\n",
      "Epoch 98 Iter 187: loss 0.11814. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.50it/s]\n",
      "Epoch 99 Iter 187: loss 0.05323. : 100%|█████████████████████████████████████████████| 187/187 [00:10<00:00, 17.37it/s]\n",
      "Epoch 100 Iter 187: loss 0.04735. : 100%|████████████████████████████████████████████| 187/187 [00:10<00:00, 17.49it/s]\n",
      "\u001b[32m[I 2021-06-08 15:11:41,023]\u001b[0m Trial 0 finished with value: 98.12714143358315 and parameters: {'batch_size': 392, 'learning_rate': 0.001793528644326713}. Best is trial 0 with value: 98.12714143358315.\u001b[0m\n",
      "Epoch 1 Iter 3: loss 0.08557. :   1%|▍                                                 | 2/209 [00:00<00:14, 14.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0022705302816287457 Batch size: 351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iter 209: loss 0.14917. : 100%|██████████████████████████████████████████████| 209/209 [00:11<00:00, 17.87it/s]\n",
      "Epoch 2 Iter 209: loss 0.09416. : 100%|██████████████████████████████████████████████| 209/209 [00:11<00:00, 17.86it/s]\n",
      "Epoch 3 Iter 209: loss 0.09897. : 100%|██████████████████████████████████████████████| 209/209 [00:11<00:00, 17.92it/s]\n",
      "Epoch 4 Iter 209: loss 0.12064. : 100%|██████████████████████████████████████████████| 209/209 [00:11<00:00, 17.91it/s]\n",
      "Epoch 5 Iter 209: loss 0.14405. : 100%|██████████████████████████████████████████████| 209/209 [00:11<00:00, 17.72it/s]\n",
      "Epoch 6 Iter 209: loss 0.15214. : 100%|██████████████████████████████████████████████| 209/209 [00:11<00:00, 17.93it/s]\n",
      "Epoch 7 Iter 209: loss 0.09144. : 100%|██████████████████████████████████████████████| 209/209 [00:11<00:00, 17.76it/s]\n",
      "Epoch 8 Iter 209: loss 0.10301. : 100%|██████████████████████████████████████████████| 209/209 [00:11<00:00, 17.73it/s]\n",
      "Epoch 9 Iter 209: loss 0.06138. : 100%|██████████████████████████████████████████████| 209/209 [00:11<00:00, 17.80it/s]\n",
      "Epoch 10 Iter 209: loss 0.07754. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.94it/s]\n",
      "Epoch 11 Iter 209: loss 0.07849. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.79it/s]\n",
      "Epoch 12 Iter 209: loss 0.09518. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.83it/s]\n",
      "Epoch 13 Iter 209: loss 0.08944. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.84it/s]\n",
      "Epoch 14 Iter 209: loss 0.07369. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.85it/s]\n",
      "Epoch 15 Iter 209: loss 0.06778. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.96it/s]\n",
      "Epoch 16 Iter 209: loss 0.17710. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.92it/s]\n",
      "Epoch 17 Iter 209: loss 0.05708. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.82it/s]\n",
      "Epoch 18 Iter 209: loss 0.08953. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.87it/s]\n",
      "Epoch 19 Iter 209: loss 0.08801. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.85it/s]\n",
      "Epoch 20 Iter 209: loss 0.04907. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.87it/s]\n",
      "Epoch 21 Iter 209: loss 0.15389. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.98it/s]\n",
      "Epoch 22 Iter 209: loss 0.04598. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.78it/s]\n",
      "Epoch 23 Iter 209: loss 0.08028. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.86it/s]\n",
      "Epoch 24 Iter 209: loss 0.05553. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.85it/s]\n",
      "Epoch 25 Iter 209: loss 0.17739. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.90it/s]\n",
      "Epoch 26 Iter 209: loss 0.14305. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.95it/s]\n",
      "Epoch 27 Iter 209: loss 0.08452. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.82it/s]\n",
      "Epoch 28 Iter 209: loss 0.06936. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.86it/s]\n",
      "Epoch 29 Iter 209: loss 0.07115. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.89it/s]\n",
      "Epoch 30 Iter 209: loss 0.12932. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.79it/s]\n",
      "Epoch 31 Iter 209: loss 0.12730. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.93it/s]\n",
      "Epoch 32 Iter 209: loss 0.09647. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.92it/s]\n",
      "Epoch 33 Iter 209: loss 0.06011. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.80it/s]\n",
      "Epoch 34 Iter 209: loss 0.05164. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.95it/s]\n",
      "Epoch 35 Iter 209: loss 0.02768. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.93it/s]\n",
      "Epoch 36 Iter 209: loss 0.06149. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.85it/s]\n",
      "Epoch 37 Iter 209: loss 0.06145. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 18.00it/s]\n",
      "Epoch 38 Iter 209: loss 0.07438. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.94it/s]\n",
      "Epoch 39 Iter 209: loss 0.12816. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.82it/s]\n",
      "Epoch 40 Iter 209: loss 0.10059. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.87it/s]\n",
      "Epoch 41 Iter 209: loss 0.02335. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.80it/s]\n",
      "Epoch 42 Iter 209: loss 0.05421. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.73it/s]\n",
      "Epoch 43 Iter 209: loss 0.11515. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.90it/s]\n",
      "Epoch 44 Iter 209: loss 0.04893. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.84it/s]\n",
      "Epoch 45 Iter 209: loss 0.05691. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.75it/s]\n",
      "Epoch 46 Iter 209: loss 0.11572. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.93it/s]\n",
      "Epoch 47 Iter 209: loss 0.11041. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.92it/s]\n",
      "Epoch 48 Iter 209: loss 0.07346. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.71it/s]\n",
      "Epoch 49 Iter 209: loss 0.05679. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.87it/s]\n",
      "Epoch 50 Iter 209: loss 0.05240. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.96it/s]\n",
      "Epoch 51 Iter 209: loss 0.08442. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.80it/s]\n",
      "Epoch 52 Iter 209: loss 0.02516. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.92it/s]\n",
      "Epoch 53 Iter 209: loss 0.07542. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.86it/s]\n",
      "Epoch 54 Iter 209: loss 0.12064. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.71it/s]\n",
      "Epoch 55 Iter 209: loss 0.07305. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.80it/s]\n",
      "Epoch 56 Iter 209: loss 0.17021. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.84it/s]\n",
      "Epoch 57 Iter 209: loss 0.05407. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.72it/s]\n",
      "Epoch 58 Iter 209: loss 0.06671. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.80it/s]\n",
      "Epoch 59 Iter 209: loss 0.07760. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.86it/s]\n",
      "Epoch 60 Iter 209: loss 0.09885. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.81it/s]\n",
      "Epoch 61 Iter 209: loss 0.07604. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.89it/s]\n",
      "Epoch 62 Iter 209: loss 0.14360. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.94it/s]\n",
      "Epoch 63 Iter 209: loss 0.07206. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.79it/s]\n",
      "Epoch 64 Iter 209: loss 0.04880. : 100%|█████████████████████████████████████████████| 209/209 [00:12<00:00, 17.38it/s]\n",
      "Epoch 65 Iter 209: loss 0.05185. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.95it/s]\n",
      "Epoch 66 Iter 209: loss 0.01148. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.85it/s]\n",
      "Epoch 67 Iter 209: loss 0.05006. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.88it/s]\n",
      "Epoch 68 Iter 209: loss 0.07652. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.93it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69 Iter 209: loss 0.06587. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.84it/s]\n",
      "Epoch 70 Iter 209: loss 0.05388. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.95it/s]\n",
      "Epoch 71 Iter 209: loss 0.08935. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.91it/s]\n",
      "Epoch 72 Iter 209: loss 0.04735. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.90it/s]\n",
      "Epoch 73 Iter 209: loss 0.09442. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.83it/s]\n",
      "Epoch 74 Iter 209: loss 0.03815. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.92it/s]\n",
      "Epoch 75 Iter 209: loss 0.09975. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.89it/s]\n",
      "Epoch 76 Iter 209: loss 0.10615. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.80it/s]\n",
      "Epoch 77 Iter 209: loss 0.11390. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.96it/s]\n",
      "Epoch 78 Iter 209: loss 0.02363. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.84it/s]\n",
      "Epoch 79 Iter 209: loss 0.06462. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.84it/s]\n",
      "Epoch 80 Iter 209: loss 0.06591. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.94it/s]\n",
      "Epoch 81 Iter 209: loss 0.09764. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.90it/s]\n",
      "Epoch 82 Iter 209: loss 0.05420. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.85it/s]\n",
      "Epoch 83 Iter 209: loss 0.05425. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.90it/s]\n",
      "Epoch 84 Iter 209: loss 0.02366. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.88it/s]\n",
      "Epoch 85 Iter 209: loss 0.04372. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.72it/s]\n",
      "Epoch 86 Iter 209: loss 0.03335. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.84it/s]\n",
      "Epoch 87 Iter 209: loss 0.02144. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.91it/s]\n",
      "Epoch 88 Iter 209: loss 0.11740. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.88it/s]\n",
      "Epoch 89 Iter 209: loss 0.10596. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.95it/s]\n",
      "Epoch 90 Iter 209: loss 0.07956. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.93it/s]\n",
      "Epoch 91 Iter 209: loss 0.01811. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.85it/s]\n",
      "Epoch 92 Iter 209: loss 0.05948. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.92it/s]\n",
      "Epoch 93 Iter 209: loss 0.07832. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.75it/s]\n",
      "Epoch 94 Iter 209: loss 0.04712. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.71it/s]\n",
      "Epoch 95 Iter 209: loss 0.10463. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.89it/s]\n",
      "Epoch 96 Iter 209: loss 0.08504. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.85it/s]\n",
      "Epoch 97 Iter 209: loss 0.09337. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.89it/s]\n",
      "Epoch 98 Iter 209: loss 0.02840. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.96it/s]\n",
      "Epoch 99 Iter 209: loss 0.03373. : 100%|█████████████████████████████████████████████| 209/209 [00:11<00:00, 17.82it/s]\n",
      "Epoch 100 Iter 209: loss 0.01419. : 100%|████████████████████████████████████████████| 209/209 [00:11<00:00, 17.82it/s]\n",
      "\u001b[32m[I 2021-06-08 15:32:31,716]\u001b[0m Trial 1 finished with value: 99.22191735943323 and parameters: {'batch_size': 351, 'learning_rate': 0.0022705302816287457}. Best is trial 1 with value: 99.22191735943323.\u001b[0m\n",
      "Epoch 1 Iter 8: loss 0.27233. :   1%|▎                                                 | 4/660 [00:00<00:17, 37.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0035827217335716527 Batch size: 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iter 660: loss 0.33629. : 100%|██████████████████████████████████████████████| 660/660 [00:12<00:00, 53.62it/s]\n",
      "Epoch 2 Iter 660: loss 0.48702. : 100%|██████████████████████████████████████████████| 660/660 [00:12<00:00, 53.63it/s]\n",
      "Epoch 3 Iter 660: loss 0.25848. : 100%|██████████████████████████████████████████████| 660/660 [00:12<00:00, 53.33it/s]\n",
      "Epoch 4 Iter 660: loss 0.34276. : 100%|██████████████████████████████████████████████| 660/660 [00:12<00:00, 53.60it/s]\n",
      "Epoch 5 Iter 660: loss 0.22392. : 100%|██████████████████████████████████████████████| 660/660 [00:12<00:00, 53.06it/s]\n",
      "Epoch 6 Iter 660: loss 0.18877. : 100%|██████████████████████████████████████████████| 660/660 [00:12<00:00, 52.43it/s]\n",
      "Epoch 7 Iter 660: loss 0.27870. : 100%|██████████████████████████████████████████████| 660/660 [00:12<00:00, 52.43it/s]\n",
      "Epoch 8 Iter 660: loss 0.32854. : 100%|██████████████████████████████████████████████| 660/660 [00:12<00:00, 52.66it/s]\n",
      "Epoch 9 Iter 660: loss 0.22927. : 100%|██████████████████████████████████████████████| 660/660 [00:12<00:00, 53.63it/s]\n",
      "Epoch 10 Iter 660: loss 0.28003. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.49it/s]\n",
      "Epoch 11 Iter 660: loss 0.16572. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.64it/s]\n",
      "Epoch 12 Iter 660: loss 0.31429. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.55it/s]\n",
      "Epoch 13 Iter 660: loss 0.35770. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.60it/s]\n",
      "Epoch 14 Iter 660: loss 0.19676. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.65it/s]\n",
      "Epoch 15 Iter 660: loss 0.31596. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.54it/s]\n",
      "Epoch 16 Iter 660: loss 0.15114. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.59it/s]\n",
      "Epoch 17 Iter 660: loss 0.26007. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.76it/s]\n",
      "Epoch 18 Iter 660: loss 0.20926. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.61it/s]\n",
      "Epoch 19 Iter 660: loss 0.15437. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.57it/s]\n",
      "Epoch 20 Iter 660: loss 0.17460. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.46it/s]\n",
      "Epoch 21 Iter 660: loss 0.18248. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.30it/s]\n",
      "Epoch 22 Iter 660: loss 0.23919. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.41it/s]\n",
      "Epoch 23 Iter 660: loss 0.29348. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.09it/s]\n",
      "Epoch 24 Iter 660: loss 0.08709. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.38it/s]\n",
      "Epoch 25 Iter 660: loss 0.37558. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.24it/s]\n",
      "Epoch 26 Iter 660: loss 0.08105. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.30it/s]\n",
      "Epoch 27 Iter 660: loss 0.18883. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.29it/s]\n",
      "Epoch 28 Iter 660: loss 0.29174. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.17it/s]\n",
      "Epoch 29 Iter 660: loss 0.18153. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.38it/s]\n",
      "Epoch 30 Iter 660: loss 0.27570. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.31it/s]\n",
      "Epoch 31 Iter 660: loss 0.17072. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.25it/s]\n",
      "Epoch 32 Iter 660: loss 0.20817. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.32it/s]\n",
      "Epoch 33 Iter 660: loss 0.15667. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.37it/s]\n",
      "Epoch 34 Iter 660: loss 0.27660. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.51it/s]\n",
      "Epoch 35 Iter 660: loss 0.21257. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.25it/s]\n",
      "Epoch 36 Iter 660: loss 0.14972. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.32it/s]\n",
      "Epoch 37 Iter 660: loss 0.10980. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 52.18it/s]\n",
      "Epoch 38 Iter 660: loss 0.17092. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.09it/s]\n",
      "Epoch 39 Iter 660: loss 0.17553. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.28it/s]\n",
      "Epoch 40 Iter 660: loss 0.14809. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.54it/s]\n",
      "Epoch 41 Iter 660: loss 0.28970. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.59it/s]\n",
      "Epoch 42 Iter 660: loss 0.45078. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.62it/s]\n",
      "Epoch 43 Iter 660: loss 0.18784. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.36it/s]\n",
      "Epoch 44 Iter 660: loss 0.36612. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.52it/s]\n",
      "Epoch 45 Iter 660: loss 0.15546. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.36it/s]\n",
      "Epoch 46 Iter 660: loss 0.16321. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.43it/s]\n",
      "Epoch 47 Iter 660: loss 0.08578. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.43it/s]\n",
      "Epoch 48 Iter 660: loss 0.19992. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.57it/s]\n",
      "Epoch 49 Iter 660: loss 0.36340. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.73it/s]\n",
      "Epoch 50 Iter 660: loss 0.38855. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.36it/s]\n",
      "Epoch 51 Iter 660: loss 0.10109. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.28it/s]\n",
      "Epoch 52 Iter 660: loss 0.18985. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.44it/s]\n",
      "Epoch 53 Iter 660: loss 0.15153. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.59it/s]\n",
      "Epoch 54 Iter 660: loss 0.14266. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.45it/s]\n",
      "Epoch 55 Iter 660: loss 0.24049. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.62it/s]\n",
      "Epoch 56 Iter 660: loss 0.26630. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.47it/s]\n",
      "Epoch 57 Iter 660: loss 0.10359. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.39it/s]\n",
      "Epoch 58 Iter 660: loss 0.12260. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.34it/s]\n",
      "Epoch 59 Iter 660: loss 0.16776. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.57it/s]\n",
      "Epoch 60 Iter 660: loss 0.28882. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.41it/s]\n",
      "Epoch 61 Iter 660: loss 0.18039. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.48it/s]\n",
      "Epoch 62 Iter 660: loss 0.06009. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.55it/s]\n",
      "Epoch 63 Iter 660: loss 0.19666. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.66it/s]\n",
      "Epoch 64 Iter 660: loss 0.11184. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.64it/s]\n",
      "Epoch 65 Iter 660: loss 0.21357. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.42it/s]\n",
      "Epoch 66 Iter 660: loss 0.14323. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.62it/s]\n",
      "Epoch 67 Iter 660: loss 0.07392. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.41it/s]\n",
      "Epoch 68 Iter 660: loss 0.34597. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.56it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69 Iter 660: loss 0.21195. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.45it/s]\n",
      "Epoch 70 Iter 660: loss 0.10432. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.47it/s]\n",
      "Epoch 71 Iter 660: loss 0.13919. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.61it/s]\n",
      "Epoch 72 Iter 660: loss 0.25966. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.39it/s]\n",
      "Epoch 73 Iter 660: loss 0.15674. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.46it/s]\n",
      "Epoch 74 Iter 660: loss 0.19725. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.69it/s]\n",
      "Epoch 75 Iter 660: loss 0.13915. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.43it/s]\n",
      "Epoch 76 Iter 660: loss 0.44248. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.47it/s]\n",
      "Epoch 77 Iter 660: loss 0.12565. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.65it/s]\n",
      "Epoch 78 Iter 660: loss 0.15286. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.59it/s]\n",
      "Epoch 79 Iter 660: loss 0.28232. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.43it/s]\n",
      "Epoch 80 Iter 660: loss 0.17719. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.68it/s]\n",
      "Epoch 81 Iter 660: loss 0.06073. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.48it/s]\n",
      "Epoch 82 Iter 660: loss 0.47664. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.59it/s]\n",
      "Epoch 83 Iter 660: loss 0.19073. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.15it/s]\n",
      "Epoch 84 Iter 660: loss 0.27111. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 52.70it/s]\n",
      "Epoch 85 Iter 660: loss 0.07550. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 52.54it/s]\n",
      "Epoch 86 Iter 660: loss 0.14407. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 52.68it/s]\n",
      "Epoch 87 Iter 660: loss 0.12618. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 52.67it/s]\n",
      "Epoch 88 Iter 660: loss 0.10531. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.33it/s]\n",
      "Epoch 89 Iter 660: loss 0.07276. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 52.92it/s]\n",
      "Epoch 90 Iter 660: loss 0.09793. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 52.96it/s]\n",
      "Epoch 91 Iter 660: loss 0.30949. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.24it/s]\n",
      "Epoch 92 Iter 660: loss 0.18182. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.12it/s]\n",
      "Epoch 93 Iter 660: loss 0.05658. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.06it/s]\n",
      "Epoch 94 Iter 660: loss 0.20610. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.08it/s]\n",
      "Epoch 95 Iter 660: loss 0.16743. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.09it/s]\n",
      "Epoch 96 Iter 660: loss 0.03006. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.12it/s]\n",
      "Epoch 97 Iter 660: loss 0.12882. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.28it/s]\n",
      "Epoch 98 Iter 660: loss 0.36449. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 53.18it/s]\n",
      "Epoch 99 Iter 660: loss 0.17403. : 100%|█████████████████████████████████████████████| 660/660 [00:12<00:00, 52.96it/s]\n",
      "Epoch 100 Iter 660: loss 0.09124. : 100%|████████████████████████████████████████████| 660/660 [00:12<00:00, 53.04it/s]\n",
      "\u001b[32m[I 2021-06-08 15:54:29,383]\u001b[0m Trial 2 finished with value: 96.07409530829818 and parameters: {'batch_size': 111, 'learning_rate': 0.0035827217335716527}. Best is trial 1 with value: 99.22191735943323.\u001b[0m\n",
      "Epoch 1 Iter 2: loss 0.14727. :   1%|▌                                                 | 2/161 [00:00<00:13, 11.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.002153380341012614 Batch size: 457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iter 161: loss 0.05262. : 100%|██████████████████████████████████████████████| 161/161 [00:11<00:00, 14.06it/s]\n",
      "Epoch 2 Iter 161: loss 0.05664. : 100%|██████████████████████████████████████████████| 161/161 [00:11<00:00, 14.09it/s]\n",
      "Epoch 3 Iter 161: loss 0.05909. : 100%|██████████████████████████████████████████████| 161/161 [00:11<00:00, 14.05it/s]\n",
      "Epoch 4 Iter 161: loss 0.05605. : 100%|██████████████████████████████████████████████| 161/161 [00:11<00:00, 14.00it/s]\n",
      "Epoch 5 Iter 161: loss 0.01100. : 100%|██████████████████████████████████████████████| 161/161 [00:11<00:00, 14.10it/s]\n",
      "Epoch 6 Iter 161: loss 0.01353. : 100%|██████████████████████████████████████████████| 161/161 [00:11<00:00, 14.12it/s]\n",
      "Epoch 7 Iter 161: loss 0.00497. : 100%|██████████████████████████████████████████████| 161/161 [00:11<00:00, 14.02it/s]\n",
      "Epoch 8 Iter 161: loss 0.04105. : 100%|██████████████████████████████████████████████| 161/161 [00:11<00:00, 13.82it/s]\n",
      "Epoch 9 Iter 161: loss 0.10876. : 100%|██████████████████████████████████████████████| 161/161 [00:11<00:00, 13.87it/s]\n",
      "Epoch 10 Iter 161: loss 0.01147. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.10it/s]\n",
      "Epoch 11 Iter 161: loss 0.01402. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.05it/s]\n",
      "Epoch 12 Iter 161: loss 0.00995. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 13.80it/s]\n",
      "Epoch 13 Iter 161: loss 0.05297. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 13.74it/s]\n",
      "Epoch 14 Iter 161: loss 0.00431. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 13.68it/s]\n",
      "Epoch 15 Iter 161: loss 0.02930. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 13.74it/s]\n",
      "Epoch 16 Iter 161: loss 0.00979. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 13.76it/s]\n",
      "Epoch 17 Iter 161: loss 0.00755. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 13.95it/s]\n",
      "Epoch 18 Iter 161: loss 0.06221. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 13.81it/s]\n",
      "Epoch 19 Iter 161: loss 0.00773. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 13.62it/s]\n",
      "Epoch 20 Iter 161: loss 0.01695. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 13.74it/s]\n",
      "Epoch 21 Iter 161: loss 0.02379. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 13.75it/s]\n",
      "Epoch 22 Iter 161: loss 0.04459. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 13.84it/s]\n",
      "Epoch 23 Iter 161: loss 0.00113. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.09it/s]\n",
      "Epoch 24 Iter 161: loss 0.00779. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.09it/s]\n",
      "Epoch 25 Iter 161: loss 0.00781. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.02it/s]\n",
      "Epoch 26 Iter 161: loss 0.01009. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.09it/s]\n",
      "Epoch 27 Iter 161: loss 0.00306. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 13.96it/s]\n",
      "Epoch 28 Iter 161: loss 0.00231. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.08it/s]\n",
      "Epoch 29 Iter 161: loss 0.00678. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.02it/s]\n",
      "Epoch 30 Iter 161: loss 0.07566. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.08it/s]\n",
      "Epoch 31 Iter 161: loss 0.01008. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 13.97it/s]\n",
      "Epoch 32 Iter 161: loss 0.03529. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.10it/s]\n",
      "Epoch 33 Iter 161: loss 0.01683. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.12it/s]\n",
      "Epoch 34 Iter 161: loss 0.00278. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.06it/s]\n",
      "Epoch 35 Iter 161: loss 0.00188. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.12it/s]\n",
      "Epoch 36 Iter 161: loss 0.00145. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.04it/s]\n",
      "Epoch 37 Iter 161: loss 0.00202. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.08it/s]\n",
      "Epoch 38 Iter 161: loss 0.00232. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.05it/s]\n",
      "Epoch 39 Iter 161: loss 0.00070. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.11it/s]\n",
      "Epoch 40 Iter 161: loss 0.01865. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.02it/s]\n",
      "Epoch 41 Iter 161: loss 0.00175. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.08it/s]\n",
      "Epoch 42 Iter 161: loss 0.08144. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.12it/s]\n",
      "Epoch 43 Iter 161: loss 0.01903. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.05it/s]\n",
      "Epoch 44 Iter 161: loss 0.03918. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.11it/s]\n",
      "Epoch 45 Iter 161: loss 0.00722. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.06it/s]\n",
      "Epoch 46 Iter 161: loss 0.00106. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.06it/s]\n",
      "Epoch 47 Iter 161: loss 0.00129. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.00it/s]\n",
      "Epoch 48 Iter 161: loss 0.00174. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.08it/s]\n",
      "Epoch 49 Iter 161: loss 0.00114. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.06it/s]\n",
      "Epoch 50 Iter 161: loss 0.00103. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.11it/s]\n",
      "Epoch 51 Iter 161: loss 0.00355. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.03it/s]\n",
      "Epoch 52 Iter 161: loss 0.00087. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.05it/s]\n",
      "Epoch 53 Iter 161: loss 0.01225. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.07it/s]\n",
      "Epoch 54 Iter 161: loss 0.02498. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.05it/s]\n",
      "Epoch 55 Iter 161: loss 0.00284. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.07it/s]\n",
      "Epoch 56 Iter 161: loss 0.01289. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.07it/s]\n",
      "Epoch 57 Iter 161: loss 0.00082. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.10it/s]\n",
      "Epoch 58 Iter 161: loss 0.00314. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.03it/s]\n",
      "Epoch 59 Iter 161: loss 0.00093. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.09it/s]\n",
      "Epoch 60 Iter 161: loss 0.00036. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.08it/s]\n",
      "Epoch 61 Iter 161: loss 0.00113. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.05it/s]\n",
      "Epoch 62 Iter 161: loss 0.00056. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.09it/s]\n",
      "Epoch 63 Iter 161: loss 0.00041. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 13.83it/s]\n",
      "Epoch 64 Iter 161: loss 0.00141. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 13.79it/s]\n",
      "Epoch 65 Iter 161: loss 0.00530. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 13.84it/s]\n",
      "Epoch 66 Iter 161: loss 0.03897. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.00it/s]\n",
      "Epoch 67 Iter 161: loss 0.01341. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.10it/s]\n",
      "Epoch 68 Iter 161: loss 0.10061. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.08it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69 Iter 161: loss 0.02054. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.08it/s]\n",
      "Epoch 70 Iter 161: loss 0.06061. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 13.93it/s]\n",
      "Epoch 71 Iter 161: loss 0.00196. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 13.70it/s]\n",
      "Epoch 72 Iter 161: loss 0.00093. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 13.66it/s]\n",
      "Epoch 73 Iter 161: loss 0.00121. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 13.75it/s]\n",
      "Epoch 74 Iter 161: loss 0.00112. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 13.73it/s]\n",
      "Epoch 75 Iter 161: loss 0.00058. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 13.73it/s]\n",
      "Epoch 76 Iter 161: loss 0.00297. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 13.99it/s]\n",
      "Epoch 77 Iter 161: loss 0.00176. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.09it/s]\n",
      "Epoch 78 Iter 161: loss 0.00212. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.08it/s]\n",
      "Epoch 79 Iter 161: loss 0.00056. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.01it/s]\n",
      "Epoch 80 Iter 161: loss 0.00017. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.00it/s]\n",
      "Epoch 81 Iter 161: loss 0.07603. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.00it/s]\n",
      "Epoch 82 Iter 161: loss 0.00705. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.08it/s]\n",
      "Epoch 83 Iter 161: loss 0.03739. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 13.94it/s]\n",
      "Epoch 84 Iter 161: loss 0.00479. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.07it/s]\n",
      "Epoch 85 Iter 161: loss 0.00817. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.07it/s]\n",
      "Epoch 86 Iter 161: loss 0.00895. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.06it/s]\n",
      "Epoch 87 Iter 161: loss 0.00142. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.01it/s]\n",
      "Epoch 88 Iter 161: loss 0.00108. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.10it/s]\n",
      "Epoch 89 Iter 161: loss 0.00170. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.06it/s]\n",
      "Epoch 90 Iter 161: loss 0.00104. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.04it/s]\n",
      "Epoch 91 Iter 161: loss 0.00116. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.09it/s]\n",
      "Epoch 92 Iter 161: loss 0.00081. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.11it/s]\n",
      "Epoch 93 Iter 161: loss 0.00015. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.08it/s]\n",
      "Epoch 94 Iter 161: loss 0.04442. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.04it/s]\n",
      "Epoch 95 Iter 161: loss 0.01929. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.09it/s]\n",
      "Epoch 96 Iter 161: loss 0.00138. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.01it/s]\n",
      "Epoch 97 Iter 161: loss 0.00271. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 13.98it/s]\n",
      "Epoch 98 Iter 161: loss 0.00185. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.08it/s]\n",
      "Epoch 99 Iter 161: loss 0.00035. : 100%|█████████████████████████████████████████████| 161/161 [00:11<00:00, 14.09it/s]\n",
      "Epoch 100 Iter 161: loss 0.00043. : 100%|████████████████████████████████████████████| 161/161 [00:11<00:00, 14.09it/s]\n",
      "\u001b[32m[I 2021-06-08 16:15:00,259]\u001b[0m Trial 3 finished with value: 99.9972698854717 and parameters: {'batch_size': 457, 'learning_rate': 0.002153380341012614}. Best is trial 3 with value: 99.9972698854717.\u001b[0m\n",
      "Epoch 1 Iter 5: loss 0.19786. :   1%|▍                                                 | 3/388 [00:00<00:15, 25.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.004947910239351018 Batch size: 189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iter 388: loss 0.23890. : 100%|██████████████████████████████████████████████| 388/388 [00:11<00:00, 32.57it/s]\n",
      "Epoch 2 Iter 388: loss 0.04053. : 100%|██████████████████████████████████████████████| 388/388 [00:11<00:00, 32.93it/s]\n",
      "Epoch 3 Iter 388: loss 0.27046. : 100%|██████████████████████████████████████████████| 388/388 [00:11<00:00, 32.99it/s]\n",
      "Epoch 4 Iter 388: loss 0.25171. : 100%|██████████████████████████████████████████████| 388/388 [00:11<00:00, 32.90it/s]\n",
      "Epoch 5 Iter 388: loss 0.29109. : 100%|██████████████████████████████████████████████| 388/388 [00:11<00:00, 32.83it/s]\n",
      "Epoch 6 Iter 388: loss 0.20066. : 100%|██████████████████████████████████████████████| 388/388 [00:11<00:00, 32.98it/s]\n",
      "Epoch 7 Iter 388: loss 0.12073. : 100%|██████████████████████████████████████████████| 388/388 [00:11<00:00, 32.95it/s]\n",
      "Epoch 8 Iter 388: loss 0.04201. : 100%|██████████████████████████████████████████████| 388/388 [00:11<00:00, 32.78it/s]\n",
      "Epoch 9 Iter 388: loss 0.12182. : 100%|██████████████████████████████████████████████| 388/388 [00:11<00:00, 32.79it/s]\n",
      "Epoch 10 Iter 388: loss 0.28825. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.37it/s]\n",
      "Epoch 11 Iter 388: loss 0.22827. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.72it/s]\n",
      "Epoch 12 Iter 388: loss 0.07656. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.94it/s]\n",
      "Epoch 13 Iter 388: loss 0.14016. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.91it/s]\n",
      "Epoch 14 Iter 388: loss 0.07826. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.87it/s]\n",
      "Epoch 15 Iter 388: loss 0.24542. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.99it/s]\n",
      "Epoch 16 Iter 388: loss 0.18450. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.58it/s]\n",
      "Epoch 17 Iter 388: loss 0.13145. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.66it/s]\n",
      "Epoch 18 Iter 388: loss 0.08717. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.90it/s]\n",
      "Epoch 19 Iter 388: loss 0.18200. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.79it/s]\n",
      "Epoch 20 Iter 388: loss 0.10724. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.73it/s]\n",
      "Epoch 21 Iter 388: loss 0.38663. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.90it/s]\n",
      "Epoch 22 Iter 388: loss 0.13522. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.77it/s]\n",
      "Epoch 23 Iter 388: loss 0.02982. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.87it/s]\n",
      "Epoch 24 Iter 388: loss 0.27337. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.70it/s]\n",
      "Epoch 25 Iter 388: loss 0.14216. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.69it/s]\n",
      "Epoch 26 Iter 388: loss 0.07520. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.80it/s]\n",
      "Epoch 27 Iter 388: loss 0.15134. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.80it/s]\n",
      "Epoch 28 Iter 388: loss 0.17568. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.82it/s]\n",
      "Epoch 29 Iter 388: loss 0.23218. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.91it/s]\n",
      "Epoch 30 Iter 388: loss 0.30595. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.45it/s]\n",
      "Epoch 31 Iter 388: loss 0.36154. : 100%|█████████████████████████████████████████████| 388/388 [00:12<00:00, 31.85it/s]\n",
      "Epoch 32 Iter 388: loss 0.23929. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.53it/s]\n",
      "Epoch 33 Iter 388: loss 0.09458. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.68it/s]\n",
      "Epoch 34 Iter 388: loss 0.15842. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.75it/s]\n",
      "Epoch 35 Iter 388: loss 0.01516. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.69it/s]\n",
      "Epoch 36 Iter 388: loss 0.13597. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.76it/s]\n",
      "Epoch 37 Iter 388: loss 0.01977. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.84it/s]\n",
      "Epoch 38 Iter 388: loss 0.11131. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.81it/s]\n",
      "Epoch 39 Iter 388: loss 0.07335. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.93it/s]\n",
      "Epoch 40 Iter 388: loss 0.20599. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.86it/s]\n",
      "Epoch 41 Iter 388: loss 0.09570. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.84it/s]\n",
      "Epoch 42 Iter 388: loss 0.29113. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.85it/s]\n",
      "Epoch 43 Iter 388: loss 0.03665. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.85it/s]\n",
      "Epoch 44 Iter 388: loss 0.26068. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.87it/s]\n",
      "Epoch 45 Iter 388: loss 0.17206. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.80it/s]\n",
      "Epoch 46 Iter 388: loss 0.14968. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.91it/s]\n",
      "Epoch 47 Iter 388: loss 0.11499. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.87it/s]\n",
      "Epoch 48 Iter 388: loss 0.22263. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.80it/s]\n",
      "Epoch 49 Iter 388: loss 0.25544. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.80it/s]\n",
      "Epoch 50 Iter 388: loss 0.04265. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.83it/s]\n",
      "Epoch 51 Iter 388: loss 0.13863. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.82it/s]\n",
      "Epoch 52 Iter 388: loss 0.03370. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.67it/s]\n",
      "Epoch 53 Iter 388: loss 0.14685. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.78it/s]\n",
      "Epoch 54 Iter 388: loss 0.17100. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.73it/s]\n",
      "Epoch 55 Iter 388: loss 0.17224. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.80it/s]\n",
      "Epoch 56 Iter 388: loss 0.29946. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.77it/s]\n",
      "Epoch 57 Iter 388: loss 0.15853. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.84it/s]\n",
      "Epoch 58 Iter 388: loss 0.20409. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.76it/s]\n",
      "Epoch 59 Iter 388: loss 0.25308. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.83it/s]\n",
      "Epoch 60 Iter 388: loss 0.14319. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.90it/s]\n",
      "Epoch 61 Iter 388: loss 0.12670. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.79it/s]\n",
      "Epoch 62 Iter 388: loss 0.10355. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.85it/s]\n",
      "Epoch 63 Iter 388: loss 0.18401. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.89it/s]\n",
      "Epoch 64 Iter 388: loss 0.17671. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.90it/s]\n",
      "Epoch 65 Iter 388: loss 0.03889. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.90it/s]\n",
      "Epoch 66 Iter 388: loss 0.17856. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.74it/s]\n",
      "Epoch 67 Iter 388: loss 0.13004. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.60it/s]\n",
      "Epoch 68 Iter 388: loss 0.15660. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.89it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69 Iter 388: loss 0.26323. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.91it/s]\n",
      "Epoch 70 Iter 388: loss 0.12198. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.97it/s]\n",
      "Epoch 71 Iter 388: loss 0.04445. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.80it/s]\n",
      "Epoch 72 Iter 388: loss 0.16468. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.77it/s]\n",
      "Epoch 73 Iter 388: loss 0.28059. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.93it/s]\n",
      "Epoch 74 Iter 388: loss 0.13263. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.99it/s]\n",
      "Epoch 75 Iter 388: loss 0.11525. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.59it/s]\n",
      "Epoch 76 Iter 388: loss 0.29698. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.93it/s]\n",
      "Epoch 77 Iter 388: loss 0.34338. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.82it/s]\n",
      "Epoch 78 Iter 388: loss 0.10134. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.88it/s]\n",
      "Epoch 79 Iter 388: loss 0.07441. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.99it/s]\n",
      "Epoch 80 Iter 388: loss 0.16324. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.96it/s]\n",
      "Epoch 81 Iter 388: loss 0.15166. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.74it/s]\n",
      "Epoch 82 Iter 388: loss 0.05243. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.90it/s]\n",
      "Epoch 83 Iter 388: loss 0.09508. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.94it/s]\n",
      "Epoch 84 Iter 388: loss 0.07150. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.98it/s]\n",
      "Epoch 85 Iter 388: loss 0.08697. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.91it/s]\n",
      "Epoch 86 Iter 388: loss 0.08147. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.83it/s]\n",
      "Epoch 87 Iter 388: loss 0.16980. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.88it/s]\n",
      "Epoch 88 Iter 388: loss 0.29981. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.98it/s]\n",
      "Epoch 89 Iter 388: loss 0.03814. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.84it/s]\n",
      "Epoch 90 Iter 388: loss 0.18523. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.91it/s]\n",
      "Epoch 91 Iter 388: loss 0.13145. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.95it/s]\n",
      "Epoch 92 Iter 388: loss 0.05141. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.76it/s]\n",
      "Epoch 93 Iter 388: loss 0.07853. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.84it/s]\n",
      "Epoch 94 Iter 388: loss 0.02006. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.96it/s]\n",
      "Epoch 95 Iter 388: loss 0.06198. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.69it/s]\n",
      "Epoch 96 Iter 388: loss 0.19466. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.87it/s]\n",
      "Epoch 97 Iter 388: loss 0.13344. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.74it/s]\n",
      "Epoch 98 Iter 388: loss 0.05078. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.59it/s]\n",
      "Epoch 99 Iter 388: loss 0.60391. : 100%|█████████████████████████████████████████████| 388/388 [00:11<00:00, 32.62it/s]\n",
      "Epoch 100 Iter 388: loss 0.20175. : 100%|████████████████████████████████████████████| 388/388 [00:11<00:00, 32.42it/s]\n",
      "\u001b[32m[I 2021-06-08 16:36:03,292]\u001b[0m Trial 4 finished with value: 96.49726306018538 and parameters: {'batch_size': 189, 'learning_rate': 0.004947910239351018}. Best is trial 3 with value: 99.9972698854717.\u001b[0m\n",
      "Epoch 1 Iter 8: loss 0.09496. :   1%|▍                                                 | 5/666 [00:00<00:16, 41.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.004612028376215349 Batch size: 110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iter 666: loss 0.06636. : 100%|██████████████████████████████████████████████| 666/666 [00:12<00:00, 54.10it/s]\n",
      "Epoch 2 Iter 666: loss 0.28607. : 100%|██████████████████████████████████████████████| 666/666 [00:12<00:00, 54.02it/s]\n",
      "Epoch 3 Iter 666: loss 0.55342. : 100%|██████████████████████████████████████████████| 666/666 [00:12<00:00, 53.93it/s]\n",
      "Epoch 4 Iter 666: loss 0.06734. : 100%|██████████████████████████████████████████████| 666/666 [00:12<00:00, 54.08it/s]\n",
      "Epoch 5 Iter 666: loss 0.18550. : 100%|██████████████████████████████████████████████| 666/666 [00:12<00:00, 53.86it/s]\n",
      "Epoch 6 Iter 666: loss 0.05014. : 100%|██████████████████████████████████████████████| 666/666 [00:12<00:00, 54.25it/s]\n",
      "Epoch 7 Iter 666: loss 0.04680. : 100%|██████████████████████████████████████████████| 666/666 [00:12<00:00, 54.22it/s]\n",
      "Epoch 8 Iter 666: loss 0.25840. : 100%|██████████████████████████████████████████████| 666/666 [00:12<00:00, 54.12it/s]\n",
      "Epoch 9 Iter 666: loss 0.17970. : 100%|██████████████████████████████████████████████| 666/666 [00:12<00:00, 54.01it/s]\n",
      "Epoch 10 Iter 666: loss 0.24183. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.94it/s]\n",
      "Epoch 11 Iter 666: loss 0.33865. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.13it/s]\n",
      "Epoch 12 Iter 666: loss 0.15569. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.07it/s]\n",
      "Epoch 13 Iter 666: loss 0.20354. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.73it/s]\n",
      "Epoch 14 Iter 666: loss 0.14209. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.04it/s]\n",
      "Epoch 15 Iter 666: loss 0.09466. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.98it/s]\n",
      "Epoch 16 Iter 666: loss 0.21971. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.24it/s]\n",
      "Epoch 17 Iter 666: loss 0.33869. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.22it/s]\n",
      "Epoch 18 Iter 666: loss 0.24471. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.82it/s]\n",
      "Epoch 19 Iter 666: loss 0.14317. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.00it/s]\n",
      "Epoch 20 Iter 666: loss 0.14346. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.11it/s]\n",
      "Epoch 21 Iter 666: loss 0.17910. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.11it/s]\n",
      "Epoch 22 Iter 666: loss 0.27896. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.12it/s]\n",
      "Epoch 23 Iter 666: loss 0.30840. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.12it/s]\n",
      "Epoch 24 Iter 666: loss 0.16469. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.84it/s]\n",
      "Epoch 25 Iter 666: loss 0.12706. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.93it/s]\n",
      "Epoch 26 Iter 666: loss 0.06689. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.10it/s]\n",
      "Epoch 27 Iter 666: loss 0.21447. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.15it/s]\n",
      "Epoch 28 Iter 666: loss 0.05463. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.18it/s]\n",
      "Epoch 29 Iter 666: loss 0.19947. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.17it/s]\n",
      "Epoch 30 Iter 666: loss 0.12698. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.98it/s]\n",
      "Epoch 31 Iter 666: loss 0.11759. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.10it/s]\n",
      "Epoch 32 Iter 666: loss 0.08901. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.05it/s]\n",
      "Epoch 33 Iter 666: loss 0.15117. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.03it/s]\n",
      "Epoch 34 Iter 666: loss 0.06261. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.05it/s]\n",
      "Epoch 35 Iter 666: loss 0.11452. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.71it/s]\n",
      "Epoch 36 Iter 666: loss 0.14907. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.01it/s]\n",
      "Epoch 37 Iter 666: loss 0.18903. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.96it/s]\n",
      "Epoch 38 Iter 666: loss 0.27687. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.09it/s]\n",
      "Epoch 39 Iter 666: loss 0.28508. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.18it/s]\n",
      "Epoch 40 Iter 666: loss 0.17400. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.02it/s]\n",
      "Epoch 41 Iter 666: loss 0.20839. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.19it/s]\n",
      "Epoch 42 Iter 666: loss 0.13100. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.02it/s]\n",
      "Epoch 43 Iter 666: loss 0.17245. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.03it/s]\n",
      "Epoch 44 Iter 666: loss 0.02797. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.10it/s]\n",
      "Epoch 45 Iter 666: loss 0.45719. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.79it/s]\n",
      "Epoch 46 Iter 666: loss 0.22984. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.07it/s]\n",
      "Epoch 47 Iter 666: loss 0.10001. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.18it/s]\n",
      "Epoch 48 Iter 666: loss 0.22495. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.14it/s]\n",
      "Epoch 49 Iter 666: loss 0.15126. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.95it/s]\n",
      "Epoch 50 Iter 666: loss 0.03470. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.93it/s]\n",
      "Epoch 51 Iter 666: loss 0.20446. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.14it/s]\n",
      "Epoch 52 Iter 666: loss 0.17839. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.16it/s]\n",
      "Epoch 53 Iter 666: loss 0.26711. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.05it/s]\n",
      "Epoch 54 Iter 666: loss 0.13954. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.12it/s]\n",
      "Epoch 55 Iter 666: loss 0.25942. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.15it/s]\n",
      "Epoch 56 Iter 666: loss 0.14832. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.16it/s]\n",
      "Epoch 57 Iter 666: loss 0.21617. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.06it/s]\n",
      "Epoch 58 Iter 666: loss 0.20759. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.18it/s]\n",
      "Epoch 59 Iter 666: loss 0.16967. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.75it/s]\n",
      "Epoch 60 Iter 666: loss 0.27662. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.12it/s]\n",
      "Epoch 61 Iter 666: loss 0.18430. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.11it/s]\n",
      "Epoch 62 Iter 666: loss 0.04142. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 52.60it/s]\n",
      "Epoch 63 Iter 666: loss 0.12240. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.78it/s]\n",
      "Epoch 64 Iter 666: loss 0.49563. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.02it/s]\n",
      "Epoch 65 Iter 666: loss 0.26371. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.05it/s]\n",
      "Epoch 66 Iter 666: loss 0.12104. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.26it/s]\n",
      "Epoch 67 Iter 666: loss 0.24047. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.61it/s]\n",
      "Epoch 68 Iter 666: loss 0.18546. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.97it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69 Iter 666: loss 0.18598. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.11it/s]\n",
      "Epoch 70 Iter 666: loss 0.27216. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.90it/s]\n",
      "Epoch 71 Iter 666: loss 0.18930. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.13it/s]\n",
      "Epoch 72 Iter 666: loss 0.19079. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.05it/s]\n",
      "Epoch 73 Iter 666: loss 0.46082. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.01it/s]\n",
      "Epoch 74 Iter 666: loss 0.26651. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.97it/s]\n",
      "Epoch 75 Iter 666: loss 0.18094. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.22it/s]\n",
      "Epoch 76 Iter 666: loss 0.22810. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 54.02it/s]\n",
      "Epoch 77 Iter 666: loss 0.48250. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.47it/s]\n",
      "Epoch 78 Iter 666: loss 0.13570. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.43it/s]\n",
      "Epoch 79 Iter 666: loss 0.38306. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.31it/s]\n",
      "Epoch 80 Iter 666: loss 0.22770. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.55it/s]\n",
      "Epoch 81 Iter 666: loss 0.36472. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.37it/s]\n",
      "Epoch 82 Iter 666: loss 0.08116. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.33it/s]\n",
      "Epoch 83 Iter 666: loss 0.04698. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.39it/s]\n",
      "Epoch 84 Iter 666: loss 0.09857. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.26it/s]\n",
      "Epoch 85 Iter 666: loss 0.14640. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.33it/s]\n",
      "Epoch 86 Iter 666: loss 0.15822. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.43it/s]\n",
      "Epoch 87 Iter 666: loss 0.23086. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.45it/s]\n",
      "Epoch 88 Iter 666: loss 0.19398. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.35it/s]\n",
      "Epoch 89 Iter 666: loss 0.23248. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.35it/s]\n",
      "Epoch 90 Iter 666: loss 0.32249. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.55it/s]\n",
      "Epoch 91 Iter 666: loss 0.16167. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.42it/s]\n",
      "Epoch 92 Iter 666: loss 0.40022. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.28it/s]\n",
      "Epoch 93 Iter 666: loss 0.41053. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.44it/s]\n",
      "Epoch 94 Iter 666: loss 0.30460. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.53it/s]\n",
      "Epoch 95 Iter 666: loss 0.26981. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.35it/s]\n",
      "Epoch 96 Iter 666: loss 0.32961. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.28it/s]\n",
      "Epoch 97 Iter 666: loss 0.12670. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 52.83it/s]\n",
      "Epoch 98 Iter 666: loss 0.22911. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.23it/s]\n",
      "Epoch 99 Iter 666: loss 0.03503. : 100%|█████████████████████████████████████████████| 666/666 [00:12<00:00, 53.39it/s]\n",
      "Epoch 100 Iter 666: loss 0.29530. : 100%|████████████████████████████████████████████| 666/666 [00:12<00:00, 53.27it/s]\n",
      "\u001b[32m[I 2021-06-08 16:58:00,144]\u001b[0m Trial 5 finished with value: 96.50408834650614 and parameters: {'batch_size': 110, 'learning_rate': 0.004612028376215349}. Best is trial 3 with value: 99.9972698854717.\u001b[0m\n",
      "Epoch 1 Iter 11: loss 0.46012. :   1%|▎                                                | 6/928 [00:00<00:16, 56.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.003327966686159006 Batch size: 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iter 928: loss 0.03387. : 100%|██████████████████████████████████████████████| 928/928 [00:12<00:00, 71.40it/s]\n",
      "Epoch 2 Iter 928: loss 0.00030. : 100%|██████████████████████████████████████████████| 928/928 [00:12<00:00, 71.61it/s]\n",
      "Epoch 3 Iter 928: loss 0.04474. : 100%|██████████████████████████████████████████████| 928/928 [00:12<00:00, 71.68it/s]\n",
      "Epoch 4 Iter 928: loss 0.02942. : 100%|██████████████████████████████████████████████| 928/928 [00:12<00:00, 71.66it/s]\n",
      "Epoch 5 Iter 928: loss 0.01530. : 100%|██████████████████████████████████████████████| 928/928 [00:12<00:00, 71.71it/s]\n",
      "Epoch 6 Iter 928: loss 0.09514. : 100%|██████████████████████████████████████████████| 928/928 [00:12<00:00, 71.57it/s]\n",
      "Epoch 7 Iter 928: loss 0.31132. : 100%|██████████████████████████████████████████████| 928/928 [00:12<00:00, 71.71it/s]\n",
      "Epoch 8 Iter 928: loss 0.00053. : 100%|██████████████████████████████████████████████| 928/928 [00:13<00:00, 71.18it/s]\n",
      "Epoch 9 Iter 928: loss 0.21713. : 100%|██████████████████████████████████████████████| 928/928 [00:12<00:00, 71.52it/s]\n",
      "Epoch 10 Iter 928: loss 0.00010. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 71.45it/s]\n",
      "Epoch 11 Iter 928: loss 0.04117. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 71.63it/s]\n",
      "Epoch 12 Iter 928: loss 0.28651. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 71.68it/s]\n",
      "Epoch 13 Iter 928: loss 0.16821. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 71.68it/s]\n",
      "Epoch 14 Iter 928: loss 0.34208. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 71.54it/s]\n",
      "Epoch 15 Iter 928: loss 0.13464. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 71.60it/s]\n",
      "Epoch 16 Iter 928: loss 0.05019. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 71.51it/s]\n",
      "Epoch 17 Iter 928: loss 0.54472. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 71.61it/s]\n",
      "Epoch 18 Iter 928: loss 0.02056. : 100%|█████████████████████████████████████████████| 928/928 [00:13<00:00, 69.53it/s]\n",
      "Epoch 19 Iter 928: loss 0.21683. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 71.91it/s]\n",
      "Epoch 20 Iter 928: loss 0.00297. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 72.50it/s]\n",
      "Epoch 21 Iter 928: loss 0.00096. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 73.68it/s]\n",
      "Epoch 22 Iter 928: loss 0.04924. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 74.04it/s]\n",
      "Epoch 23 Iter 928: loss 0.00013. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 73.83it/s]\n",
      "Epoch 24 Iter 928: loss 0.02850. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 73.95it/s]\n",
      "Epoch 25 Iter 928: loss 0.18269. : 100%|█████████████████████████████████████████████| 928/928 [00:13<00:00, 69.64it/s]\n",
      "Epoch 26 Iter 928: loss 0.86990. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 71.76it/s]\n",
      "Epoch 27 Iter 928: loss 0.00860. : 100%|█████████████████████████████████████████████| 928/928 [00:14<00:00, 64.62it/s]\n",
      "Epoch 28 Iter 928: loss 0.12429. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 73.29it/s]\n",
      "Epoch 29 Iter 928: loss 0.10525. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 73.56it/s]\n",
      "Epoch 30 Iter 928: loss 0.00227. : 100%|█████████████████████████████████████████████| 928/928 [00:13<00:00, 70.16it/s]\n",
      "Epoch 31 Iter 928: loss 0.21275. : 100%|█████████████████████████████████████████████| 928/928 [00:13<00:00, 70.56it/s]\n",
      "Epoch 32 Iter 928: loss 0.00006. : 100%|█████████████████████████████████████████████| 928/928 [00:13<00:00, 71.29it/s]\n",
      "Epoch 33 Iter 928: loss 0.00000. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 73.86it/s]\n",
      "Epoch 34 Iter 928: loss 0.15973. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 73.74it/s]\n",
      "Epoch 35 Iter 928: loss 0.06043. : 100%|█████████████████████████████████████████████| 928/928 [00:13<00:00, 70.86it/s]\n",
      "Epoch 36 Iter 928: loss 0.02062. : 100%|█████████████████████████████████████████████| 928/928 [00:13<00:00, 69.65it/s]\n",
      "Epoch 37 Iter 928: loss 0.04820. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 73.53it/s]\n",
      "Epoch 38 Iter 928: loss 0.12088. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 73.69it/s]\n",
      "Epoch 39 Iter 928: loss 0.69794. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 73.88it/s]\n",
      "Epoch 40 Iter 928: loss 0.26924. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 74.12it/s]\n",
      "Epoch 41 Iter 928: loss 0.42802. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 72.20it/s]\n",
      "Epoch 42 Iter 928: loss 0.00047. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 72.83it/s]\n",
      "Epoch 43 Iter 928: loss 0.33903. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 73.44it/s]\n",
      "Epoch 44 Iter 928: loss 0.07237. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 73.40it/s]\n",
      "Epoch 45 Iter 928: loss 0.18271. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 74.13it/s]\n",
      "Epoch 46 Iter 928: loss 0.08816. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 73.86it/s]\n",
      "Epoch 47 Iter 928: loss 0.00420. : 100%|█████████████████████████████████████████████| 928/928 [00:13<00:00, 68.51it/s]\n",
      "Epoch 48 Iter 928: loss 0.01476. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 72.15it/s]\n",
      "Epoch 49 Iter 928: loss 0.07608. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 74.08it/s]\n",
      "Epoch 50 Iter 928: loss 0.24838. : 100%|█████████████████████████████████████████████| 928/928 [00:13<00:00, 70.62it/s]\n",
      "Epoch 51 Iter 928: loss 0.00009. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 73.78it/s]\n",
      "Epoch 52 Iter 928: loss 0.33271. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 73.70it/s]\n",
      "Epoch 53 Iter 928: loss 0.25500. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 73.05it/s]\n",
      "Epoch 54 Iter 928: loss 0.00078. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 73.49it/s]\n",
      "Epoch 55 Iter 928: loss 0.01583. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 73.34it/s]\n",
      "Epoch 56 Iter 928: loss 0.00000. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 73.76it/s]\n",
      "Epoch 57 Iter 928: loss 0.00521. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 73.66it/s]\n",
      "Epoch 58 Iter 928: loss 0.49869. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 71.75it/s]\n",
      "Epoch 59 Iter 928: loss 0.27574. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 71.60it/s]\n",
      "Epoch 60 Iter 928: loss 0.02463. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 73.62it/s]\n",
      "Epoch 61 Iter 928: loss 0.33079. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 73.11it/s]\n",
      "Epoch 62 Iter 928: loss 1.03424. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 72.70it/s]\n",
      "Epoch 63 Iter 928: loss 0.09389. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 71.84it/s]\n",
      "Epoch 64 Iter 928: loss 0.23586. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 72.85it/s]\n",
      "Epoch 65 Iter 928: loss 0.05021. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 73.98it/s]\n",
      "Epoch 66 Iter 928: loss 0.28599. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 73.79it/s]\n",
      "Epoch 67 Iter 928: loss 0.00012. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 73.86it/s]\n",
      "Epoch 68 Iter 928: loss 0.29555. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 73.99it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69 Iter 928: loss 1.24117. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 74.03it/s]\n",
      "Epoch 70 Iter 928: loss 0.18825. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 74.01it/s]\n",
      "Epoch 71 Iter 928: loss 0.08223. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 73.75it/s]\n",
      "Epoch 72 Iter 928: loss 0.02278. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 73.10it/s]\n",
      "Epoch 73 Iter 928: loss 0.01071. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 73.11it/s]\n",
      "Epoch 74 Iter 928: loss 0.00188. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 73.07it/s]\n",
      "Epoch 75 Iter 928: loss 0.00157. : 100%|█████████████████████████████████████████████| 928/928 [00:13<00:00, 70.45it/s]\n",
      "Epoch 76 Iter 928: loss 0.13484. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 73.15it/s]\n",
      "Epoch 77 Iter 928: loss 0.00278. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 74.07it/s]\n",
      "Epoch 78 Iter 928: loss 0.12993. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 74.12it/s]\n",
      "Epoch 79 Iter 928: loss 0.00023. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 74.07it/s]\n",
      "Epoch 80 Iter 928: loss 0.34298. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 74.02it/s]\n",
      "Epoch 81 Iter 928: loss 0.01449. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 73.84it/s]\n",
      "Epoch 82 Iter 928: loss 0.03920. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 74.04it/s]\n",
      "Epoch 83 Iter 928: loss 0.00047. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 73.85it/s]\n",
      "Epoch 84 Iter 928: loss 0.18530. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 72.12it/s]\n",
      "Epoch 85 Iter 928: loss 0.00002. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 71.74it/s]\n",
      "Epoch 86 Iter 928: loss 0.00070. : 100%|█████████████████████████████████████████████| 928/928 [00:13<00:00, 70.84it/s]\n",
      "Epoch 87 Iter 928: loss 0.17994. : 100%|█████████████████████████████████████████████| 928/928 [00:13<00:00, 71.14it/s]\n",
      "Epoch 88 Iter 928: loss 0.22936. : 100%|█████████████████████████████████████████████| 928/928 [00:13<00:00, 68.53it/s]\n",
      "Epoch 89 Iter 928: loss 0.05993. : 100%|█████████████████████████████████████████████| 928/928 [00:14<00:00, 62.97it/s]\n",
      "Epoch 90 Iter 928: loss 0.28133. : 100%|█████████████████████████████████████████████| 928/928 [00:13<00:00, 68.52it/s]\n",
      "Epoch 91 Iter 928: loss 0.00822. : 100%|█████████████████████████████████████████████| 928/928 [00:13<00:00, 67.90it/s]\n",
      "Epoch 92 Iter 928: loss 0.00021. : 100%|█████████████████████████████████████████████| 928/928 [00:14<00:00, 63.70it/s]\n",
      "Epoch 93 Iter 928: loss 0.00011. : 100%|█████████████████████████████████████████████| 928/928 [00:14<00:00, 63.93it/s]\n",
      "Epoch 94 Iter 928: loss 0.00321. : 100%|█████████████████████████████████████████████| 928/928 [00:13<00:00, 68.41it/s]\n",
      "Epoch 95 Iter 928: loss 0.00207. : 100%|█████████████████████████████████████████████| 928/928 [00:13<00:00, 70.97it/s]\n",
      "Epoch 96 Iter 928: loss 0.02830. : 100%|█████████████████████████████████████████████| 928/928 [00:13<00:00, 69.73it/s]\n",
      "Epoch 97 Iter 928: loss 0.00007. : 100%|█████████████████████████████████████████████| 928/928 [00:13<00:00, 70.79it/s]\n",
      "Epoch 98 Iter 928: loss 0.00318. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 71.65it/s]\n",
      "Epoch 99 Iter 928: loss 0.00811. : 100%|█████████████████████████████████████████████| 928/928 [00:12<00:00, 72.90it/s]\n",
      "Epoch 100 Iter 928: loss 0.02374. : 100%|████████████████████████████████████████████| 928/928 [00:12<00:00, 73.03it/s]\n",
      "\u001b[32m[I 2021-06-08 17:20:51,317]\u001b[0m Trial 6 finished with value: 97.52651623735615 and parameters: {'batch_size': 79, 'learning_rate': 0.003327966686159006}. Best is trial 3 with value: 99.9972698854717.\u001b[0m\n",
      "Epoch 1 Iter 24: loss 0.44333. :   0%|▏                                             | 13/2931 [00:00<00:23, 123.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.004514959200589319 Batch size: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iter 2931: loss 0.01458. : 100%|██████████████████████████████████████████| 2931/2931 [00:19<00:00, 149.36it/s]\n",
      "Epoch 2 Iter 2931: loss 0.37266. : 100%|██████████████████████████████████████████| 2931/2931 [00:19<00:00, 150.56it/s]\n",
      "Epoch 3 Iter 2931: loss 0.81595. : 100%|██████████████████████████████████████████| 2931/2931 [00:19<00:00, 150.55it/s]\n",
      "Epoch 4 Iter 2931: loss 0.02811. : 100%|██████████████████████████████████████████| 2931/2931 [00:19<00:00, 150.17it/s]\n",
      "Epoch 5 Iter 2931: loss 1.92346. : 100%|██████████████████████████████████████████| 2931/2931 [00:19<00:00, 150.66it/s]\n",
      "Epoch 6 Iter 2931: loss 0.92066. : 100%|██████████████████████████████████████████| 2931/2931 [00:19<00:00, 150.53it/s]\n",
      "Epoch 7 Iter 2931: loss 0.65792. : 100%|██████████████████████████████████████████| 2931/2931 [00:19<00:00, 147.33it/s]\n",
      "Epoch 8 Iter 2931: loss 0.51252. : 100%|██████████████████████████████████████████| 2931/2931 [00:19<00:00, 148.71it/s]\n",
      "Epoch 9 Iter 2931: loss 0.25027. : 100%|██████████████████████████████████████████| 2931/2931 [00:21<00:00, 136.46it/s]\n",
      "Epoch 10 Iter 2931: loss 0.50977. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.50it/s]\n",
      "Epoch 11 Iter 2931: loss 0.81187. : 100%|█████████████████████████████████████████| 2931/2931 [00:21<00:00, 137.01it/s]\n",
      "Epoch 12 Iter 2931: loss 1.86244. : 100%|█████████████████████████████████████████| 2931/2931 [00:21<00:00, 134.82it/s]\n",
      "Epoch 13 Iter 2931: loss 0.00284. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.30it/s]\n",
      "Epoch 14 Iter 2931: loss 0.03160. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.83it/s]\n",
      "Epoch 15 Iter 2931: loss 0.55555. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 142.22it/s]\n",
      "Epoch 16 Iter 2931: loss 0.48156. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.95it/s]\n",
      "Epoch 17 Iter 2931: loss 0.66821. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.60it/s]\n",
      "Epoch 18 Iter 2931: loss 0.39114. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.75it/s]\n",
      "Epoch 19 Iter 2931: loss 0.04327. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.81it/s]\n",
      "Epoch 20 Iter 2931: loss 0.01473. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.68it/s]\n",
      "Epoch 21 Iter 2931: loss 0.31782. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.72it/s]\n",
      "Epoch 22 Iter 2931: loss 0.00002. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.02it/s]\n",
      "Epoch 23 Iter 2931: loss 0.20557. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.32it/s]\n",
      "Epoch 24 Iter 2931: loss 0.25181. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.62it/s]\n",
      "Epoch 25 Iter 2931: loss 0.35218. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.58it/s]\n",
      "Epoch 26 Iter 2931: loss 0.03582. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 140.37it/s]\n",
      "Epoch 27 Iter 2931: loss 0.22909. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.69it/s]\n",
      "Epoch 28 Iter 2931: loss 0.20204. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.43it/s]\n",
      "Epoch 29 Iter 2931: loss 0.86012. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.84it/s]\n",
      "Epoch 30 Iter 2931: loss 0.29189. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 140.97it/s]\n",
      "Epoch 31 Iter 2931: loss 0.82259. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.82it/s]\n",
      "Epoch 32 Iter 2931: loss 0.08983. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.59it/s]\n",
      "Epoch 33 Iter 2931: loss 0.26505. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.67it/s]\n",
      "Epoch 34 Iter 2931: loss 0.00174. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.50it/s]\n",
      "Epoch 35 Iter 2931: loss 0.26463. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.85it/s]\n",
      "Epoch 36 Iter 2931: loss 0.17012. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.37it/s]\n",
      "Epoch 37 Iter 2931: loss 0.27383. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.62it/s]\n",
      "Epoch 38 Iter 2931: loss 0.00001. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.94it/s]\n",
      "Epoch 39 Iter 2931: loss 0.12466. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.87it/s]\n",
      "Epoch 40 Iter 2931: loss 0.11320. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.88it/s]\n",
      "Epoch 41 Iter 2931: loss 0.02037. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.68it/s]\n",
      "Epoch 42 Iter 2931: loss 0.08629. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.71it/s]\n",
      "Epoch 43 Iter 2931: loss 1.74140. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.64it/s]\n",
      "Epoch 44 Iter 2931: loss 1.09060. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.87it/s]\n",
      "Epoch 45 Iter 2931: loss 0.03215. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.55it/s]\n",
      "Epoch 46 Iter 2931: loss 0.11738. : 100%|█████████████████████████████████████████| 2931/2931 [00:21<00:00, 138.35it/s]\n",
      "Epoch 47 Iter 2931: loss 1.12171. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.33it/s]\n",
      "Epoch 48 Iter 2931: loss 0.47263. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.54it/s]\n",
      "Epoch 49 Iter 2931: loss 0.00281. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.84it/s]\n",
      "Epoch 50 Iter 2931: loss 0.53363. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.33it/s]\n",
      "Epoch 51 Iter 2931: loss 1.41316. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.16it/s]\n",
      "Epoch 52 Iter 2931: loss 0.45596. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.00it/s]\n",
      "Epoch 53 Iter 2931: loss 0.21370. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.27it/s]\n",
      "Epoch 54 Iter 2931: loss 0.17928. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.79it/s]\n",
      "Epoch 55 Iter 2931: loss 1.01622. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 140.99it/s]\n",
      "Epoch 56 Iter 2931: loss 0.36650. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.53it/s]\n",
      "Epoch 57 Iter 2931: loss 2.27845. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.08it/s]\n",
      "Epoch 58 Iter 2931: loss 0.05458. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 143.52it/s]\n",
      "Epoch 59 Iter 2931: loss 0.19264. : 100%|█████████████████████████████████████████| 2931/2931 [00:19<00:00, 148.10it/s]\n",
      "Epoch 60 Iter 2931: loss 0.01084. : 100%|█████████████████████████████████████████| 2931/2931 [00:19<00:00, 148.16it/s]\n",
      "Epoch 61 Iter 2931: loss 0.70439. : 100%|█████████████████████████████████████████| 2931/2931 [00:19<00:00, 148.99it/s]\n",
      "Epoch 62 Iter 2931: loss 0.00896. : 100%|█████████████████████████████████████████| 2931/2931 [00:19<00:00, 148.55it/s]\n",
      "Epoch 63 Iter 2931: loss 0.03241. : 100%|█████████████████████████████████████████| 2931/2931 [00:19<00:00, 148.37it/s]\n",
      "Epoch 64 Iter 2931: loss 0.15258. : 100%|█████████████████████████████████████████| 2931/2931 [00:19<00:00, 148.55it/s]\n",
      "Epoch 65 Iter 2931: loss 0.29739. : 100%|█████████████████████████████████████████| 2931/2931 [00:19<00:00, 148.50it/s]\n",
      "Epoch 66 Iter 2931: loss 0.12210. : 100%|█████████████████████████████████████████| 2931/2931 [00:19<00:00, 147.64it/s]\n",
      "Epoch 67 Iter 2931: loss 0.06406. : 100%|█████████████████████████████████████████| 2931/2931 [00:19<00:00, 148.46it/s]\n",
      "Epoch 68 Iter 2931: loss 0.25898. : 100%|█████████████████████████████████████████| 2931/2931 [00:19<00:00, 148.34it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69 Iter 2931: loss 0.00034. : 100%|█████████████████████████████████████████| 2931/2931 [00:19<00:00, 148.65it/s]\n",
      "Epoch 70 Iter 2931: loss 0.00275. : 100%|█████████████████████████████████████████| 2931/2931 [00:19<00:00, 147.04it/s]\n",
      "Epoch 71 Iter 2931: loss 0.00441. : 100%|█████████████████████████████████████████| 2931/2931 [00:19<00:00, 148.67it/s]\n",
      "Epoch 72 Iter 2931: loss 0.30408. : 100%|█████████████████████████████████████████| 2931/2931 [00:19<00:00, 147.68it/s]\n",
      "Epoch 73 Iter 2931: loss 0.02198. : 100%|█████████████████████████████████████████| 2931/2931 [00:19<00:00, 147.95it/s]\n",
      "Epoch 74 Iter 2931: loss 1.45093. : 100%|█████████████████████████████████████████| 2931/2931 [00:19<00:00, 147.84it/s]\n",
      "Epoch 75 Iter 2931: loss 0.06376. : 100%|█████████████████████████████████████████| 2931/2931 [00:19<00:00, 148.07it/s]\n",
      "Epoch 76 Iter 2931: loss 0.17943. : 100%|█████████████████████████████████████████| 2931/2931 [00:19<00:00, 148.31it/s]\n",
      "Epoch 77 Iter 2931: loss 0.10157. : 100%|█████████████████████████████████████████| 2931/2931 [00:19<00:00, 148.42it/s]\n",
      "Epoch 78 Iter 2931: loss 0.05208. : 100%|█████████████████████████████████████████| 2931/2931 [00:19<00:00, 148.11it/s]\n",
      "Epoch 79 Iter 2931: loss 0.07234. : 100%|█████████████████████████████████████████| 2931/2931 [00:19<00:00, 148.71it/s]\n",
      "Epoch 80 Iter 2931: loss 0.00992. : 100%|█████████████████████████████████████████| 2931/2931 [00:19<00:00, 148.40it/s]\n",
      "Epoch 81 Iter 2931: loss 0.01039. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 142.60it/s]\n",
      "Epoch 82 Iter 2931: loss 0.11055. : 100%|█████████████████████████████████████████| 2931/2931 [00:19<00:00, 148.47it/s]\n",
      "Epoch 83 Iter 2931: loss 0.00637. : 100%|█████████████████████████████████████████| 2931/2931 [00:19<00:00, 148.63it/s]\n",
      "Epoch 84 Iter 2931: loss 0.62917. : 100%|█████████████████████████████████████████| 2931/2931 [00:19<00:00, 148.30it/s]\n",
      "Epoch 85 Iter 2931: loss 0.62937. : 100%|█████████████████████████████████████████| 2931/2931 [00:19<00:00, 147.32it/s]\n",
      "Epoch 86 Iter 2931: loss 0.00677. : 100%|█████████████████████████████████████████| 2931/2931 [00:19<00:00, 147.76it/s]\n",
      "Epoch 87 Iter 2931: loss 0.18002. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 146.09it/s]\n",
      "Epoch 88 Iter 2931: loss 0.86239. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 140.44it/s]\n",
      "Epoch 89 Iter 2931: loss 0.44912. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.31it/s]\n",
      "Epoch 90 Iter 2931: loss 0.16435. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 140.78it/s]\n",
      "Epoch 91 Iter 2931: loss 0.24939. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.33it/s]\n",
      "Epoch 92 Iter 2931: loss 0.19203. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.08it/s]\n",
      "Epoch 93 Iter 2931: loss 0.55992. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.09it/s]\n",
      "Epoch 94 Iter 2931: loss 0.31232. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.72it/s]\n",
      "Epoch 95 Iter 2931: loss 0.17022. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.81it/s]\n",
      "Epoch 96 Iter 2931: loss 0.13758. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.63it/s]\n",
      "Epoch 97 Iter 2931: loss 0.33636. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.71it/s]\n",
      "Epoch 98 Iter 2931: loss 0.60848. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.85it/s]\n",
      "Epoch 99 Iter 2931: loss 0.22341. : 100%|█████████████████████████████████████████| 2931/2931 [00:20<00:00, 141.23it/s]\n",
      "Epoch 100 Iter 2931: loss 0.01020. : 100%|████████████████████████████████████████| 2931/2931 [00:20<00:00, 142.49it/s]\n",
      "\u001b[32m[I 2021-06-08 17:56:10,701]\u001b[0m Trial 7 finished with value: 92.36523472159656 and parameters: {'batch_size': 25, 'learning_rate': 0.004514959200589319}. Best is trial 3 with value: 99.9972698854717.\u001b[0m\n",
      "Epoch 1 Iter 6: loss 0.16299. :   1%|▎                                                 | 3/429 [00:00<00:15, 27.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0010714956643203296 Batch size: 171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iter 429: loss 0.15279. : 100%|██████████████████████████████████████████████| 429/429 [00:12<00:00, 33.82it/s]\n",
      "Epoch 2 Iter 429: loss 0.10389. : 100%|██████████████████████████████████████████████| 429/429 [00:12<00:00, 33.64it/s]\n",
      "Epoch 3 Iter 429: loss 0.14226. : 100%|██████████████████████████████████████████████| 429/429 [00:12<00:00, 33.74it/s]\n",
      "Epoch 4 Iter 429: loss 0.02704. : 100%|██████████████████████████████████████████████| 429/429 [00:12<00:00, 33.76it/s]\n",
      "Epoch 5 Iter 429: loss 0.03066. : 100%|██████████████████████████████████████████████| 429/429 [00:12<00:00, 34.31it/s]\n",
      "Epoch 6 Iter 429: loss 0.06739. : 100%|██████████████████████████████████████████████| 429/429 [00:12<00:00, 34.19it/s]\n",
      "Epoch 7 Iter 429: loss 0.08758. : 100%|██████████████████████████████████████████████| 429/429 [00:12<00:00, 34.30it/s]\n",
      "Epoch 8 Iter 429: loss 0.19961. : 100%|██████████████████████████████████████████████| 429/429 [00:12<00:00, 34.53it/s]\n",
      "Epoch 9 Iter 429: loss 0.18562. : 100%|██████████████████████████████████████████████| 429/429 [00:12<00:00, 34.46it/s]\n",
      "Epoch 10 Iter 429: loss 0.04676. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.42it/s]\n",
      "Epoch 11 Iter 429: loss 0.11597. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.46it/s]\n",
      "Epoch 12 Iter 429: loss 0.08055. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.56it/s]\n",
      "Epoch 13 Iter 429: loss 0.08911. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.18it/s]\n",
      "Epoch 14 Iter 429: loss 0.08883. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.47it/s]\n",
      "Epoch 15 Iter 429: loss 0.05506. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.16it/s]\n",
      "Epoch 16 Iter 429: loss 0.03164. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 33.81it/s]\n",
      "Epoch 17 Iter 429: loss 0.01217. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 33.40it/s]\n",
      "Epoch 18 Iter 429: loss 0.10424. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 33.72it/s]\n",
      "Epoch 19 Iter 429: loss 0.06690. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 33.78it/s]\n",
      "Epoch 20 Iter 429: loss 0.08371. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 33.58it/s]\n",
      "Epoch 21 Iter 429: loss 0.02446. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 33.83it/s]\n",
      "Epoch 22 Iter 429: loss 0.05318. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 33.77it/s]\n",
      "Epoch 23 Iter 429: loss 0.03500. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 33.83it/s]\n",
      "Epoch 24 Iter 429: loss 0.13153. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.52it/s]\n",
      "Epoch 25 Iter 429: loss 0.03342. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.53it/s]\n",
      "Epoch 26 Iter 429: loss 0.09919. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.59it/s]\n",
      "Epoch 27 Iter 429: loss 0.07102. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.48it/s]\n",
      "Epoch 28 Iter 429: loss 0.22009. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.47it/s]\n",
      "Epoch 29 Iter 429: loss 0.02405. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.51it/s]\n",
      "Epoch 30 Iter 429: loss 0.03818. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.56it/s]\n",
      "Epoch 31 Iter 429: loss 0.02855. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.55it/s]\n",
      "Epoch 32 Iter 429: loss 0.02524. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.38it/s]\n",
      "Epoch 33 Iter 429: loss 0.01219. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.58it/s]\n",
      "Epoch 34 Iter 429: loss 0.04976. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.39it/s]\n",
      "Epoch 35 Iter 429: loss 0.05157. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.27it/s]\n",
      "Epoch 36 Iter 429: loss 0.01886. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.48it/s]\n",
      "Epoch 37 Iter 429: loss 0.04979. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.46it/s]\n",
      "Epoch 38 Iter 429: loss 0.09496. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.50it/s]\n",
      "Epoch 39 Iter 429: loss 0.00471. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.53it/s]\n",
      "Epoch 40 Iter 429: loss 0.00066. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.55it/s]\n",
      "Epoch 41 Iter 429: loss 0.01446. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.34it/s]\n",
      "Epoch 42 Iter 429: loss 0.02134. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.46it/s]\n",
      "Epoch 43 Iter 429: loss 0.01882. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.53it/s]\n",
      "Epoch 44 Iter 429: loss 0.03241. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.44it/s]\n",
      "Epoch 45 Iter 429: loss 0.00743. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.39it/s]\n",
      "Epoch 46 Iter 429: loss 0.02658. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.55it/s]\n",
      "Epoch 47 Iter 429: loss 0.00451. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.50it/s]\n",
      "Epoch 48 Iter 429: loss 0.00419. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.54it/s]\n",
      "Epoch 49 Iter 429: loss 0.05311. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.49it/s]\n",
      "Epoch 50 Iter 429: loss 0.00066. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.39it/s]\n",
      "Epoch 51 Iter 429: loss 0.09251. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.41it/s]\n",
      "Epoch 52 Iter 429: loss 0.04139. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 33.97it/s]\n",
      "Epoch 53 Iter 429: loss 0.02544. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.24it/s]\n",
      "Epoch 54 Iter 429: loss 0.00879. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.44it/s]\n",
      "Epoch 55 Iter 429: loss 0.03107. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.55it/s]\n",
      "Epoch 56 Iter 429: loss 0.03236. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.46it/s]\n",
      "Epoch 57 Iter 429: loss 0.01290. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.44it/s]\n",
      "Epoch 58 Iter 429: loss 0.07116. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.31it/s]\n",
      "Epoch 59 Iter 429: loss 0.03752. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.45it/s]\n",
      "Epoch 60 Iter 429: loss 0.00080. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.56it/s]\n",
      "Epoch 61 Iter 429: loss 0.02264. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.54it/s]\n",
      "Epoch 62 Iter 429: loss 0.00592. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.46it/s]\n",
      "Epoch 63 Iter 429: loss 0.00267. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.40it/s]\n",
      "Epoch 64 Iter 429: loss 0.01980. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.47it/s]\n",
      "Epoch 65 Iter 429: loss 0.03207. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.29it/s]\n",
      "Epoch 66 Iter 429: loss 0.01658. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.42it/s]\n",
      "Epoch 67 Iter 429: loss 0.01722. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.47it/s]\n",
      "Epoch 68 Iter 429: loss 0.00836. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.37it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69 Iter 429: loss 0.00091. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.47it/s]\n",
      "Epoch 70 Iter 429: loss 0.00645. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.38it/s]\n",
      "Epoch 71 Iter 429: loss 0.01683. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.32it/s]\n",
      "Epoch 72 Iter 429: loss 0.00095. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.40it/s]\n",
      "Epoch 73 Iter 429: loss 0.00671. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.31it/s]\n",
      "Epoch 74 Iter 429: loss 0.00667. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.45it/s]\n",
      "Epoch 75 Iter 429: loss 0.01148. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.51it/s]\n",
      "Epoch 76 Iter 429: loss 0.01446. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.27it/s]\n",
      "Epoch 77 Iter 429: loss 0.01638. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 33.87it/s]\n",
      "Epoch 78 Iter 429: loss 0.01136. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.47it/s]\n",
      "Epoch 79 Iter 429: loss 0.00267. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.40it/s]\n",
      "Epoch 80 Iter 429: loss 0.00350. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.49it/s]\n",
      "Epoch 81 Iter 429: loss 0.00037. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.41it/s]\n",
      "Epoch 82 Iter 429: loss 0.00196. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.36it/s]\n",
      "Epoch 83 Iter 429: loss 0.02267. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.34it/s]\n",
      "Epoch 84 Iter 429: loss 0.00281. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.40it/s]\n",
      "Epoch 85 Iter 429: loss 0.00574. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.48it/s]\n",
      "Epoch 86 Iter 429: loss 0.02950. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.37it/s]\n",
      "Epoch 87 Iter 429: loss 0.00711. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.42it/s]\n",
      "Epoch 88 Iter 429: loss 0.00037. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.41it/s]\n",
      "Epoch 89 Iter 429: loss 0.00511. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.37it/s]\n",
      "Epoch 90 Iter 429: loss 0.01411. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.47it/s]\n",
      "Epoch 91 Iter 429: loss 0.00214. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.40it/s]\n",
      "Epoch 92 Iter 429: loss 0.00060. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.39it/s]\n",
      "Epoch 93 Iter 429: loss 0.00170. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.43it/s]\n",
      "Epoch 94 Iter 429: loss 0.01423. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.53it/s]\n",
      "Epoch 95 Iter 429: loss 0.00098. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 34.42it/s]\n",
      "Epoch 96 Iter 429: loss 0.00029. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 33.54it/s]\n",
      "Epoch 97 Iter 429: loss 0.01604. : 100%|█████████████████████████████████████████████| 429/429 [00:12<00:00, 33.48it/s]\n",
      "Epoch 98 Iter 429: loss 0.00178. : 100%|█████████████████████████████████████████████| 429/429 [00:16<00:00, 25.33it/s]\n",
      "Epoch 99 Iter 429: loss 0.00540. : 100%|█████████████████████████████████████████████| 429/429 [00:30<00:00, 13.93it/s]\n",
      "Epoch 100 Iter 429: loss 0.00454. : 100%|████████████████████████████████████████████| 429/429 [00:19<00:00, 21.93it/s]\n",
      "\u001b[32m[I 2021-06-08 18:18:52,139]\u001b[0m Trial 8 finished with value: 99.874414731698 and parameters: {'batch_size': 171, 'learning_rate': 0.0010714956643203296}. Best is trial 3 with value: 99.9972698854717.\u001b[0m\n",
      "Epoch 1 Iter 7: loss 0.03205. :   1%|▍                                                 | 4/499 [00:00<00:12, 39.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0034462609647517356 Batch size: 147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iter 499: loss 0.04801. : 100%|██████████████████████████████████████████████| 499/499 [00:11<00:00, 42.13it/s]\n",
      "Epoch 2 Iter 499: loss 0.01890. : 100%|██████████████████████████████████████████████| 499/499 [00:11<00:00, 41.69it/s]\n",
      "Epoch 3 Iter 499: loss 0.02434. : 100%|██████████████████████████████████████████████| 499/499 [00:11<00:00, 42.13it/s]\n",
      "Epoch 4 Iter 499: loss 0.00209. : 100%|██████████████████████████████████████████████| 499/499 [00:11<00:00, 41.81it/s]\n",
      "Epoch 5 Iter 499: loss 0.00111. : 100%|██████████████████████████████████████████████| 499/499 [00:12<00:00, 41.32it/s]\n",
      "Epoch 6 Iter 499: loss 0.20999. : 100%|██████████████████████████████████████████████| 499/499 [00:12<00:00, 41.10it/s]\n",
      "Epoch 7 Iter 499: loss 0.01458. : 100%|██████████████████████████████████████████████| 499/499 [00:12<00:00, 41.17it/s]\n",
      "Epoch 8 Iter 499: loss 0.01056. : 100%|██████████████████████████████████████████████| 499/499 [00:12<00:00, 41.24it/s]\n",
      "Epoch 9 Iter 499: loss 0.08324. : 100%|██████████████████████████████████████████████| 499/499 [00:12<00:00, 41.21it/s]\n",
      "Epoch 10 Iter 499: loss 0.08819. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 41.19it/s]\n",
      "Epoch 11 Iter 499: loss 0.00136. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 41.01it/s]\n",
      "Epoch 12 Iter 499: loss 0.01629. : 100%|█████████████████████████████████████████████| 499/499 [00:11<00:00, 41.70it/s]\n",
      "Epoch 13 Iter 499: loss 0.02507. : 100%|█████████████████████████████████████████████| 499/499 [00:11<00:00, 42.02it/s]\n",
      "Epoch 14 Iter 499: loss 0.13488. : 100%|█████████████████████████████████████████████| 499/499 [00:11<00:00, 41.76it/s]\n",
      "Epoch 15 Iter 499: loss 0.25995. : 100%|█████████████████████████████████████████████| 499/499 [00:11<00:00, 41.63it/s]\n",
      "Epoch 16 Iter 499: loss 0.11321. : 100%|█████████████████████████████████████████████| 499/499 [00:11<00:00, 41.70it/s]\n",
      "Epoch 17 Iter 499: loss 0.25324. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 38.52it/s]\n",
      "Epoch 18 Iter 499: loss 0.33804. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 40.26it/s]\n",
      "Epoch 19 Iter 499: loss 0.00017. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 41.15it/s]\n",
      "Epoch 20 Iter 499: loss 0.03730. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 41.26it/s]\n",
      "Epoch 21 Iter 499: loss 0.00061. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 41.18it/s]\n",
      "Epoch 22 Iter 499: loss 0.21344. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 40.36it/s]\n",
      "Epoch 23 Iter 499: loss 0.16201. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 40.28it/s]\n",
      "Epoch 24 Iter 499: loss 0.11452. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 40.39it/s]\n",
      "Epoch 25 Iter 499: loss 0.00046. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 40.45it/s]\n",
      "Epoch 26 Iter 499: loss 0.00095. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 40.43it/s]\n",
      "Epoch 27 Iter 499: loss 0.01327. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 40.40it/s]\n",
      "Epoch 28 Iter 499: loss 0.03936. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 40.37it/s]\n",
      "Epoch 29 Iter 499: loss 0.02727. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 40.35it/s]\n",
      "Epoch 30 Iter 499: loss 0.33953. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 40.93it/s]\n",
      "Epoch 31 Iter 499: loss 0.26912. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 40.85it/s]\n",
      "Epoch 32 Iter 499: loss 0.00415. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 40.99it/s]\n",
      "Epoch 33 Iter 499: loss 0.17863. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 40.98it/s]\n",
      "Epoch 34 Iter 499: loss 0.09661. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 41.04it/s]\n",
      "Epoch 35 Iter 499: loss 0.00462. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 41.07it/s]\n",
      "Epoch 36 Iter 499: loss 0.19472. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 41.10it/s]\n",
      "Epoch 37 Iter 499: loss 0.00522. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 41.03it/s]\n",
      "Epoch 38 Iter 499: loss 0.09293. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 40.77it/s]\n",
      "Epoch 39 Iter 499: loss 0.00140. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 40.99it/s]\n",
      "Epoch 40 Iter 499: loss 0.01944. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 41.03it/s]\n",
      "Epoch 41 Iter 499: loss 0.10414. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 41.06it/s]\n",
      "Epoch 42 Iter 499: loss 0.02235. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 41.09it/s]\n",
      "Epoch 43 Iter 499: loss 0.10966. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 41.26it/s]\n",
      "Epoch 44 Iter 499: loss 0.12978. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 40.98it/s]\n",
      "Epoch 45 Iter 499: loss 0.03320. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 41.15it/s]\n",
      "Epoch 46 Iter 499: loss 0.00360. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 40.93it/s]\n",
      "Epoch 47 Iter 499: loss 0.02153. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 41.14it/s]\n",
      "Epoch 48 Iter 499: loss 0.02486. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 40.95it/s]\n",
      "Epoch 49 Iter 499: loss 0.02362. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 41.03it/s]\n",
      "Epoch 50 Iter 499: loss 0.17814. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 40.69it/s]\n",
      "Epoch 51 Iter 499: loss 0.00074. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 40.10it/s]\n",
      "Epoch 52 Iter 499: loss 0.04350. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 40.16it/s]\n",
      "Epoch 53 Iter 499: loss 0.00159. : 100%|█████████████████████████████████████████████| 499/499 [00:13<00:00, 35.70it/s]\n",
      "Epoch 54 Iter 499: loss 0.26074. : 100%|█████████████████████████████████████████████| 499/499 [00:13<00:00, 36.79it/s]\n",
      "Epoch 55 Iter 499: loss 0.18406. : 100%|█████████████████████████████████████████████| 499/499 [00:13<00:00, 37.70it/s]\n",
      "Epoch 56 Iter 499: loss 0.05230. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 38.90it/s]\n",
      "Epoch 57 Iter 499: loss 0.00345. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.12it/s]\n",
      "Epoch 58 Iter 499: loss 0.04587. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.14it/s]\n",
      "Epoch 59 Iter 499: loss 0.01515. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.09it/s]\n",
      "Epoch 60 Iter 499: loss 0.00343. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.32it/s]\n",
      "Epoch 61 Iter 499: loss 0.18105. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.11it/s]\n",
      "Epoch 62 Iter 499: loss 0.01461. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.32it/s]\n",
      "Epoch 63 Iter 499: loss 0.20286. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.32it/s]\n",
      "Epoch 64 Iter 499: loss 0.08430. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.54it/s]\n",
      "Epoch 65 Iter 499: loss 0.03686. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.47it/s]\n",
      "Epoch 66 Iter 499: loss 0.33365. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.32it/s]\n",
      "Epoch 67 Iter 499: loss 0.05299. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.40it/s]\n",
      "Epoch 68 Iter 499: loss 0.01430. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.29it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69 Iter 499: loss 0.08077. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.36it/s]\n",
      "Epoch 70 Iter 499: loss 0.03438. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.39it/s]\n",
      "Epoch 71 Iter 499: loss 0.02438. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.38it/s]\n",
      "Epoch 72 Iter 499: loss 0.28909. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.37it/s]\n",
      "Epoch 73 Iter 499: loss 0.00010. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.43it/s]\n",
      "Epoch 74 Iter 499: loss 0.00390. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.23it/s]\n",
      "Epoch 75 Iter 499: loss 0.09917. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.38it/s]\n",
      "Epoch 76 Iter 499: loss 0.06412. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.21it/s]\n",
      "Epoch 77 Iter 499: loss 0.10012. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.22it/s]\n",
      "Epoch 78 Iter 499: loss 0.00286. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.47it/s]\n",
      "Epoch 79 Iter 499: loss 0.02431. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.24it/s]\n",
      "Epoch 80 Iter 499: loss 0.07166. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.34it/s]\n",
      "Epoch 81 Iter 499: loss 0.06835. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.22it/s]\n",
      "Epoch 82 Iter 499: loss 0.00944. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.47it/s]\n",
      "Epoch 83 Iter 499: loss 0.05033. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.42it/s]\n",
      "Epoch 84 Iter 499: loss 0.06102. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.35it/s]\n",
      "Epoch 85 Iter 499: loss 0.08471. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.33it/s]\n",
      "Epoch 86 Iter 499: loss 0.04932. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.53it/s]\n",
      "Epoch 87 Iter 499: loss 0.00103. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.25it/s]\n",
      "Epoch 88 Iter 499: loss 0.10344. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.34it/s]\n",
      "Epoch 89 Iter 499: loss 0.00060. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.30it/s]\n",
      "Epoch 90 Iter 499: loss 0.09202. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.22it/s]\n",
      "Epoch 91 Iter 499: loss 0.08590. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 38.76it/s]\n",
      "Epoch 92 Iter 499: loss 0.27384. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 38.75it/s]\n",
      "Epoch 93 Iter 499: loss 0.01789. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 38.51it/s]\n",
      "Epoch 94 Iter 499: loss 0.08600. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 38.58it/s]\n",
      "Epoch 95 Iter 499: loss 0.04302. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 38.77it/s]\n",
      "Epoch 96 Iter 499: loss 0.00912. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 38.71it/s]\n",
      "Epoch 97 Iter 499: loss 0.00457. : 100%|█████████████████████████████████████████████| 499/499 [00:12<00:00, 39.01it/s]\n",
      "Epoch 98 Iter 499: loss 0.00329. : 100%|█████████████████████████████████████████████| 499/499 [00:13<00:00, 37.39it/s]\n",
      "Epoch 99 Iter 499: loss 0.09971. : 100%|█████████████████████████████████████████████| 499/499 [00:13<00:00, 37.40it/s]\n",
      "Epoch 100 Iter 499: loss 0.15767. : 100%|████████████████████████████████████████████| 499/499 [00:12<00:00, 39.06it/s]\n",
      "\u001b[32m[I 2021-06-08 18:41:14,886]\u001b[0m Trial 9 finished with value: 98.46431057782874 and parameters: {'batch_size': 147, 'learning_rate': 0.0034462609647517356}. Best is trial 3 with value: 99.9972698854717.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study best learning rate: 0.002153380341012614\n",
      "Study best batch size: 457\n"
     ]
    }
   ],
   "source": [
    "model_cnn = CNN()\n",
    "model_cnn = model_cnn.to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# Optimize learning rate and batch size\n",
    "def objective(trial):\n",
    "\n",
    "    batch_size = trial.suggest_int('batch_size', 4, 512)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 5e-3)\n",
    "\n",
    "    return train_with_optimizer(100, model_cnn, \"cnn\", criterion, learning_rate=learning_rate, batch_size=batch_size)\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "best_learning_rate = study.best_params['learning_rate']\n",
    "best_batch_size = study.best_params['batch_size']\n",
    "print(\"Study best learning rate:\", best_learning_rate)\n",
    "print(\"Study best batch size:\", best_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = LogisticRegressionModel()\n",
    "model_lr = model_lr.to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# Optimize learning rate and batch size\n",
    "def objective(trial):\n",
    "\n",
    "    batch_size = trial.suggest_int('batch_size', 4, 512)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 5e-3)\n",
    "\n",
    "    return train_with_optimizer(100, model_lr, \"lr\", criterion, learning_rate=learning_rate, batch_size=batch_size)\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "best_learning_rate = study.best_params['learning_rate']\n",
    "best_batch_size = study.best_params['batch_size']\n",
    "print(\"Study best learning rate:\", best_learning_rate)\n",
    "print(\"Study best batch size:\", best_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-sellers",
   "metadata": {},
   "source": [
    "The last two cells have ran on server, and we just report their result and continue with the given batch size and learning rate.\n",
    "They have both optimized with the same trials, and both models have same order of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-plymouth",
   "metadata": {},
   "source": [
    "the best batch size and learning rate are 362 and 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "quarterly-forum",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iter 203: loss 1.45831. : 100%|██████████████████████████████████████████████| 203/203 [00:12<00:00, 16.74it/s]\n",
      "Epoch 2 Iter 203: loss 0.94561. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.00it/s]\n",
      "Epoch 3 Iter 203: loss 0.73017. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.12it/s]\n",
      "Epoch 4 Iter 203: loss 0.65254. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 16.93it/s]\n",
      "Epoch 5 Iter 203: loss 0.66520. : 100%|██████████████████████████████████████████████| 203/203 [00:12<00:00, 16.87it/s]\n",
      "Epoch 6 Iter 203: loss 0.60716. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 16.95it/s]\n",
      "Epoch 7 Iter 203: loss 0.37081. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.22it/s]\n",
      "Epoch 8 Iter 203: loss 0.54041. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.39it/s]\n",
      "Epoch 9 Iter 203: loss 0.46126. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.33it/s]\n",
      "Epoch 10 Iter 203: loss 0.36527. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.22it/s]\n",
      "Epoch 11 Iter 203: loss 0.58124. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.29it/s]\n",
      "Epoch 12 Iter 203: loss 0.37318. : 100%|█████████████████████████████████████████████| 203/203 [00:12<00:00, 16.80it/s]\n",
      "Epoch 13 Iter 203: loss 0.45061. : 100%|█████████████████████████████████████████████| 203/203 [00:12<00:00, 16.80it/s]\n",
      "Epoch 14 Iter 203: loss 0.36428. : 100%|█████████████████████████████████████████████| 203/203 [00:12<00:00, 16.91it/s]\n",
      "Epoch 15 Iter 203: loss 0.45240. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.03it/s]\n",
      "Epoch 16 Iter 203: loss 0.53726. : 100%|█████████████████████████████████████████████| 203/203 [00:12<00:00, 16.80it/s]\n",
      "Epoch 17 Iter 203: loss 0.46891. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.02it/s]\n",
      "Epoch 18 Iter 203: loss 0.35217. : 100%|█████████████████████████████████████████████| 203/203 [00:12<00:00, 16.76it/s]\n",
      "Epoch 19 Iter 203: loss 0.50837. : 100%|█████████████████████████████████████████████| 203/203 [00:12<00:00, 16.66it/s]\n",
      "Epoch 20 Iter 203: loss 0.30249. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.37339200828169367]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training CNN with the optimized hyperparameters from Assignment 2\n",
    "model_cnn_0 = CNN()\n",
    "model_cnn_0 = model_cnn_0.to(device)\n",
    "learning_rate = 5e-4\n",
    "batch_size = 362\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model_cnn_0.parameters(), lr=learning_rate)\n",
    "model_cnn_trainer_0 = CNN_Trainer(model_cnn_0, criterion, optimizer,1, batch_size, learning_rate)\n",
    "model_cnn_trainer_0.train(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "executive-vault",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89.42217126008435, 0.37041595465113397)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnn_trainer_0.eval_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd1ed5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iter 203: loss 2.19888. : 100%|██████████████████████████████████████████████| 203/203 [00:08<00:00, 25.36it/s]\n",
      "Epoch 2 Iter 203: loss 1.94811. : 100%|██████████████████████████████████████████████| 203/203 [00:08<00:00, 24.92it/s]\n",
      "Epoch 3 Iter 203: loss 1.76054. : 100%|██████████████████████████████████████████████| 203/203 [00:08<00:00, 25.20it/s]\n",
      "Epoch 4 Iter 203: loss 1.42133. : 100%|██████████████████████████████████████████████| 203/203 [00:08<00:00, 25.02it/s]\n",
      "Epoch 5 Iter 203: loss 1.38172. : 100%|██████████████████████████████████████████████| 203/203 [00:08<00:00, 25.13it/s]\n",
      "Epoch 6 Iter 203: loss 1.21086. : 100%|██████████████████████████████████████████████| 203/203 [00:08<00:00, 24.84it/s]\n",
      "Epoch 7 Iter 203: loss 1.17925. : 100%|██████████████████████████████████████████████| 203/203 [00:07<00:00, 25.39it/s]\n",
      "Epoch 8 Iter 203: loss 1.02505. : 100%|██████████████████████████████████████████████| 203/203 [00:07<00:00, 25.48it/s]\n",
      "Epoch 9 Iter 203: loss 1.10546. : 100%|██████████████████████████████████████████████| 203/203 [00:08<00:00, 25.32it/s]\n",
      "Epoch 10 Iter 203: loss 1.17440. : 100%|█████████████████████████████████████████████| 203/203 [00:07<00:00, 25.52it/s]\n",
      "Epoch 11 Iter 203: loss 1.15746. : 100%|█████████████████████████████████████████████| 203/203 [00:07<00:00, 25.38it/s]\n",
      "Epoch 12 Iter 203: loss 0.96406. : 100%|█████████████████████████████████████████████| 203/203 [00:08<00:00, 25.27it/s]\n",
      "Epoch 13 Iter 203: loss 1.12850. : 100%|█████████████████████████████████████████████| 203/203 [00:07<00:00, 25.44it/s]\n",
      "Epoch 14 Iter 203: loss 1.15384. : 100%|█████████████████████████████████████████████| 203/203 [00:08<00:00, 25.32it/s]\n",
      "Epoch 15 Iter 203: loss 1.03883. : 100%|█████████████████████████████████████████████| 203/203 [00:08<00:00, 25.32it/s]\n",
      "Epoch 16 Iter 203: loss 0.90834. : 100%|█████████████████████████████████████████████| 203/203 [00:07<00:00, 25.53it/s]\n",
      "Epoch 17 Iter 203: loss 0.90112. : 100%|█████████████████████████████████████████████| 203/203 [00:07<00:00, 25.47it/s]\n",
      "Epoch 18 Iter 203: loss 0.98637. : 100%|█████████████████████████████████████████████| 203/203 [00:08<00:00, 25.31it/s]\n",
      "Epoch 19 Iter 203: loss 1.00974. : 100%|█████████████████████████████████████████████| 203/203 [00:07<00:00, 25.49it/s]\n",
      "Epoch 20 Iter 203: loss 1.22585. : 100%|█████████████████████████████████████████████| 203/203 [00:08<00:00, 25.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9233945446648621]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Net from assignment 2 with the optimized hyperparameters\n",
    "model_lr_0 = LogisticRegressionModel()\n",
    "model_lr_0 = model_lr_0.to(device)\n",
    "learning_rate = 5e-4\n",
    "batch_size = 362\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model_lr_0.parameters(), lr=learning_rate)\n",
    "model_lr_trainer_0 = LR_Trainer(model_lr_0, criterion, optimizer,1, batch_size, learning_rate)\n",
    "model_lr_trainer_0.train(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdce184d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73.59160216771093, 0.9044520151779578)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr_trainer_0.eval_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-valuation",
   "metadata": {},
   "source": [
    "plotting kernels and activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dangerous-newark",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_filters_multi_channel(t):\n",
    "    \n",
    "    #get the number of kernals\n",
    "    num_kernels = t.shape[0]    \n",
    "    \n",
    "    #define number of columns for subplots\n",
    "    num_cols = 12\n",
    "    #rows = num of kernels\n",
    "    num_rows = num_kernels\n",
    "    \n",
    "    #set the figure size\n",
    "    fig = plt.figure(figsize=(num_cols,num_rows))\n",
    "    #looping through all the kernels\n",
    "    for i in range(t.shape[0]):\n",
    "        ax1 = fig.add_subplot(num_rows,num_cols,i+1)\n",
    "        \n",
    "        #for each kernel, we convert the tensor to numpy \n",
    "        npimg = np.array(t[i].cpu().detach().numpy(), np.float32)\n",
    "        #standardize the numpy image\n",
    "        npimg = (npimg - np.mean(npimg)) / np.std(npimg)\n",
    "        npimg = np.minimum(1, np.maximum(0, (npimg + 0.5)))\n",
    "        npimg = npimg.transpose((1, 2, 0))\n",
    "        ax1.imshow(npimg)\n",
    "        ax1.axis('off')\n",
    "        ax1.set_title(str(i))\n",
    "        ax1.set_xticklabels([])\n",
    "        ax1.set_yticklabels([])\n",
    "        \n",
    "    plt.savefig('myimage.png', dpi=100)    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eleven-defense",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x2304 with 32 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#kernels\n",
    "kernels = model_cnn_0.layer1[0].weight\n",
    "plot_filters_multi_channel(kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "casual-satin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN with Average pooling\n",
    "class CNN_Avg(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Model initializer \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # layer 1\n",
    "        conv1 = nn.Conv2d(in_channels=3, out_channels=32,  kernel_size=3, stride=1, padding=0)\n",
    "        relu1 = nn.ReLU()\n",
    "        maxpool1 = nn.AvgPool2d(kernel_size=2)\n",
    "        self.layer1 = nn.Sequential(\n",
    "                conv1, relu1, maxpool1\n",
    "            )\n",
    "      \n",
    "        # layer 2\n",
    "        conv2 = nn.Conv2d(in_channels= 32, out_channels=64,  kernel_size=3, stride=1, padding=0)\n",
    "        relu2 = nn.ReLU()\n",
    "        maxpool2 = nn.AvgPool2d(kernel_size=2)\n",
    "        self.layer2 = nn.Sequential(\n",
    "                conv2, relu2, maxpool2\n",
    "            )\n",
    "        \n",
    "        # layer 3\n",
    "        conv3 = nn.Conv2d(in_channels=64, out_channels=128,  kernel_size=3, stride=1, padding=0)\n",
    "        relu3 = nn.ReLU()\n",
    "        maxpool3 = nn.AvgPool2d(kernel_size=2)\n",
    "        self.layer3 = nn.Sequential(\n",
    "                conv3, relu3, maxpool3\n",
    "            )\n",
    "                \n",
    "        # fully connected classifier\n",
    "        in_dim = 128 * 2 * 2\n",
    "        self.fc = nn.Linear(in_features=in_dim, out_features=10)\n",
    "\n",
    "        return\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass \"\"\"\n",
    "        cur_b_size = x.shape[0]\n",
    "        out1 = self.layer1(x)\n",
    "        out2 = self.layer2(out1)\n",
    "        out3 = self.layer3(out2)\n",
    "        out3_flat = out3.view(cur_b_size, -1)\n",
    "        y = self.fc(out3_flat)\n",
    "        return y\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "foreign-parts",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iter 203: loss 1.46895. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.05it/s]\n",
      "Epoch 2 Iter 203: loss 0.82937. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.25it/s]\n",
      "Epoch 3 Iter 203: loss 0.71690. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.41it/s]\n",
      "Epoch 4 Iter 203: loss 0.43536. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.26it/s]\n",
      "Epoch 5 Iter 203: loss 0.40488. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.46it/s]\n",
      "Epoch 6 Iter 203: loss 0.49170. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.38it/s]\n",
      "Epoch 7 Iter 203: loss 0.38689. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.31it/s]\n",
      "Epoch 8 Iter 203: loss 0.27424. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.41it/s]\n",
      "Epoch 9 Iter 203: loss 0.37270. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.10it/s]\n",
      "Epoch 10 Iter 203: loss 0.38912. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.28it/s]\n",
      "Epoch 11 Iter 203: loss 0.30810. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.08it/s]\n",
      "Epoch 12 Iter 203: loss 0.38334. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.05it/s]\n",
      "Epoch 13 Iter 203: loss 0.30259. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 16.97it/s]\n",
      "Epoch 14 Iter 203: loss 0.39697. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.01it/s]\n",
      "Epoch 15 Iter 203: loss 0.16304. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.00it/s]\n",
      "Epoch 16 Iter 203: loss 0.26045. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.04it/s]\n",
      "Epoch 17 Iter 203: loss 0.31550. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 16.97it/s]\n",
      "Epoch 18 Iter 203: loss 0.20034. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.40it/s]\n",
      "Epoch 19 Iter 203: loss 0.34571. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.49it/s]\n",
      "Epoch 20 Iter 203: loss 0.24548. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2740116889400435]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train a cnn with average pool\n",
    "cnn_avg = CNN_Avg()\n",
    "cnn_avg = cnn_avg.to(device)\n",
    "learning_rate = 5e-3\n",
    "batch_size = 362\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(cnn_avg.parameters(), lr=learning_rate)\n",
    "cnn_avg_trainer = CNN_Trainer(cnn_avg, criterion, optimizer,1, batch_size, learning_rate)\n",
    "cnn_avg_trainer.train(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "intense-share",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92.52358136423823, 0.2580550367528989)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluate loss and accuracy of model with average pooling\n",
    "cnn_avg_trainer.eval_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "olive-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN with Dropouts\n",
    "class CNN_DropOut(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Model initializer \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # layer 1\n",
    "        conv1 = nn.Conv2d(in_channels=3, out_channels=32,  kernel_size=3, stride=1, padding=0)\n",
    "        torch.nn.Dropout(0.2)\n",
    "        relu1 = nn.ReLU()\n",
    "        maxpool1 = nn.AvgPool2d(kernel_size=2)\n",
    "        self.layer1 = nn.Sequential(\n",
    "                conv1, relu1, maxpool1\n",
    "            )\n",
    "      \n",
    "        # layer 2\n",
    "        conv2 = nn.Conv2d(in_channels= 32, out_channels=64,  kernel_size=3, stride=1, padding=0)\n",
    "        torch.nn.Dropout(0.2)\n",
    "        relu2 = nn.ReLU()\n",
    "        maxpool2 = nn.AvgPool2d(kernel_size=2)\n",
    "        self.layer2 = nn.Sequential(\n",
    "                conv2, relu2, maxpool2\n",
    "            )\n",
    "        \n",
    "        # layer 3\n",
    "        conv3 = nn.Conv2d(in_channels=64, out_channels=128,  kernel_size=3, stride=1, padding=0)\n",
    "        torch.nn.Dropout(0.2)\n",
    "        relu3 = nn.ReLU()\n",
    "        maxpool3 = nn.AvgPool2d(kernel_size=2)\n",
    "        self.layer3 = nn.Sequential(\n",
    "                conv3, relu3, maxpool3\n",
    "            )\n",
    "                \n",
    "        # fully connected classifier\n",
    "        in_dim = 128 * 2 * 2\n",
    "        self.fc = nn.Linear(in_features=in_dim, out_features=10)\n",
    "\n",
    "        return\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass \"\"\"\n",
    "        cur_b_size = x.shape[0]\n",
    "        out1 = self.layer1(x)\n",
    "        out2 = self.layer2(out1)\n",
    "        out3 = self.layer3(out2)\n",
    "        out3_flat = out3.view(cur_b_size, -1)\n",
    "        y = self.fc(out3_flat)\n",
    "        return y\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "interested-formula",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iter 203: loss 1.71660. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.43it/s]\n",
      "Epoch 2 Iter 203: loss 0.78010. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.41it/s]\n",
      "Epoch 3 Iter 203: loss 0.86223. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.11it/s]\n",
      "Epoch 4 Iter 203: loss 0.57827. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.36it/s]\n",
      "Epoch 5 Iter 203: loss 0.43655. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.23it/s]\n",
      "Epoch 6 Iter 203: loss 0.63032. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.38it/s]\n",
      "Epoch 7 Iter 203: loss 0.46503. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.42it/s]\n",
      "Epoch 8 Iter 203: loss 0.43637. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.35it/s]\n",
      "Epoch 9 Iter 203: loss 0.31794. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.15it/s]\n",
      "Epoch 10 Iter 203: loss 0.39425. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.43it/s]\n",
      "Epoch 11 Iter 203: loss 0.36483. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.41it/s]\n",
      "Epoch 12 Iter 203: loss 0.39885. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.38it/s]\n",
      "Epoch 13 Iter 203: loss 0.45425. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.41it/s]\n",
      "Epoch 14 Iter 203: loss 0.37222. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.33it/s]\n",
      "Epoch 15 Iter 203: loss 0.37867. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.04it/s]\n",
      "Epoch 16 Iter 203: loss 0.25020. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.35it/s]\n",
      "Epoch 17 Iter 203: loss 0.40074. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.36it/s]\n",
      "Epoch 18 Iter 203: loss 0.38390. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.39it/s]\n",
      "Epoch 19 Iter 203: loss 0.41516. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.40it/s]\n",
      "Epoch 20 Iter 203: loss 0.31173. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.35533048777744686]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train a cnn with dropouts\n",
    "cnn_drp = CNN_DropOut()\n",
    "cnn_drp = cnn_drp.to(device)\n",
    "learning_rate = 5e-3\n",
    "batch_size = 362\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(cnn_drp.parameters(), lr=learning_rate)\n",
    "cnn_drp_trainer = CNN_Trainer(cnn_drp, criterion, optimizer,1, batch_size, learning_rate)\n",
    "cnn_drp_trainer.train(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "functioning-soundtrack",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89.8412438401791, 0.3444431540971344)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate loss and accuracy of model with dropout\n",
    "cnn_drp_trainer.eval_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-journal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare number of parameters with and without dropout\n",
    "def count_model_params(model):\n",
    "    \"\"\" Counting the number of learnable parameters in a nn.Module \"\"\"\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return num_params\n",
    "\n",
    "params = count_model_params(model_cnn_trainer_0)\n",
    "print(f\"Model without regularization has {params} learnable parameters\")\n",
    "\n",
    "params = count_model_params(cnn_drp)\n",
    "print(f\"Model with dropout has {params} learnable parameters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a33947a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN with regularization\n",
    "class CNN_Trainer_reg:\n",
    "    \n",
    "    def __init__(self, model, criterion, optimizer, save_freq, batch_size, learning_rate, reg = '', reg_lambda = 1):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.save_freq = save_freq\n",
    "        self.reg = reg\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.stats = {\n",
    "        \"epoch\": [],\n",
    "        \"train_loss\": [],\n",
    "        }\n",
    "        self.train_loader = torch.utils.data.DataLoader(trainset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "        self.test_loader=torch.utils.data.DataLoader(trainset,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=True)\n",
    "    def train(self, n_epochs, init_epoch=0):\n",
    "        loss_hist  = []\n",
    "        for epoch in range(init_epoch, n_epochs):\n",
    "            loss_list = []\n",
    "            progress_bar = tqdm(enumerate(self.train_loader), total=len(self.train_loader))\n",
    "            for i, (images, labels) in progress_bar:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "        \n",
    "                # Clear gradients w.r.t. parameters\n",
    "                self.optimizer.zero_grad()\n",
    "         \n",
    "                # Forward pass to get output/logits\n",
    "                outputs = self.model(images)\n",
    "         \n",
    "                # Calculate Loss: softmax --> cross entropy loss\n",
    "                loss = self.regularizer(self.criterion(outputs, labels))\n",
    "                loss_list.append(loss.item())\n",
    "         \n",
    "                # Getting gradients w.r.t. parameters\n",
    "                loss.backward()\n",
    "                     \n",
    "                # Updating parameters\n",
    "                self.optimizer.step()\n",
    "                progress_bar.set_description(f\"Epoch {epoch+1} Iter {i+1}: loss {loss.item():.5f}. \")\n",
    "             \n",
    "        loss_hist.append(np.mean(loss_list))\n",
    "        self.stats[\"epoch\"].append(epoch)\n",
    "        self.stats[\"train_loss\"].append(loss_hist[-1])\n",
    "    \n",
    "        ## saving checkpoint\n",
    "        #if epoch % self.save_freq == 0:\n",
    "        #    save_model(model=cnn, optimizer=optimizer, epoch=epoch, stats=stats)\n",
    "        return loss_hist\n",
    "    \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def eval_model(self):\n",
    "        \"\"\" Computing model accuracy \"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loss_list = []\n",
    "    \n",
    "        for images, labels in self.test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "        \n",
    "            # Forward pass only to get logits/output\n",
    "            outputs = self.model(images)\n",
    "\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss_list.append(loss.item())\n",
    "            \n",
    "            # Get predictions from the maximum value\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += len( torch.where(preds==labels)[0] )\n",
    "            total += len(labels)\n",
    "                 \n",
    "        # Total correct predictions and loss\n",
    "        accuracy = correct / total * 100\n",
    "        loss = np.mean(loss_list)\n",
    "        return accuracy, loss\n",
    "\n",
    "\n",
    "    def save_model(self, optimizer, epoch, stats):\n",
    "        \"\"\" Saving model checkpoint \"\"\"\n",
    "    \n",
    "        if(not os.path.exists(\"models\")):\n",
    "            os.makedirs(\"models\")\n",
    "        savepath = f\"models/checkpoint_epoch_{epoch}.pth\"\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'stats': stats\n",
    "        }, savepath)\n",
    "        return\n",
    "    \n",
    "    def regularizer(self, loss):\n",
    "        l1_regularization, l2_reg = torch.FloatTensor(0), torch.FloatTensor(0)\n",
    "        l1_regularization = l1_regularization.to(device)\n",
    "        l2_reg = l2_reg.to(device)        \n",
    "        \n",
    "        if self.reg == '':\n",
    "\n",
    "            return loss\n",
    "\n",
    "        if self.reg == 'l1':\n",
    "            regularization_loss = 0\n",
    "            for param in self.model.parameters():\n",
    "                regularization_loss += torch.sum(torch.abs(param))\n",
    "            loss += self.reg_lambda * regularization_loss\n",
    "            \n",
    "            return loss\n",
    "        if self.reg == 'l2':\n",
    "            l2_regularization = torch.tensor([0], dtype=torch.float32) # Define L2 regularization loss\n",
    "            l2_regularization = l2_regularization.to(device)\n",
    "            for param in self.model.parameters():\n",
    "                l2_regularization += torch.norm(param, 2) # L2 regularization\n",
    "            loss = loss + self.reg_lambda * l2_regularization # L2 regularization\n",
    "\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7eb0c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iter 2: loss 2.32795. :   1%|▍                                                 | 2/203 [00:00<00:15, 13.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.005\n",
      "Batch size:  362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iter 203: loss 0.64021. : 100%|██████████████████████████████████████████████| 203/203 [00:12<00:00, 16.76it/s]\n",
      "Epoch 2 Iter 203: loss 0.64155. : 100%|██████████████████████████████████████████████| 203/203 [00:12<00:00, 16.84it/s]\n",
      "Epoch 3 Iter 203: loss 0.50155. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.09it/s]\n",
      "Epoch 4 Iter 203: loss 0.46303. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.13it/s]\n",
      "Epoch 5 Iter 203: loss 0.49832. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.04it/s]\n",
      "Epoch 6 Iter 203: loss 0.48109. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.16it/s]\n",
      "Epoch 7 Iter 203: loss 0.53137. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.25it/s]\n",
      "Epoch 8 Iter 203: loss 0.73009. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.52it/s]\n",
      "Epoch 9 Iter 203: loss 0.34325. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.58it/s]\n",
      "Epoch 10 Iter 203: loss 0.50125. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.54it/s]\n",
      "Epoch 11 Iter 203: loss 0.36758. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 16.96it/s]\n",
      "Epoch 12 Iter 203: loss 0.35515. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.56it/s]\n",
      "Epoch 13 Iter 203: loss 0.23319. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.13it/s]\n",
      "Epoch 14 Iter 203: loss 0.37848. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 16.97it/s]\n",
      "Epoch 15 Iter 203: loss 0.28498. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.32it/s]\n",
      "Epoch 16 Iter 203: loss 0.26429. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.17it/s]\n",
      "Epoch 17 Iter 203: loss 0.28198. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.42it/s]\n",
      "Epoch 18 Iter 203: loss 0.43670. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.35it/s]\n",
      "Epoch 19 Iter 203: loss 0.23165. : 100%|█████████████████████████████████████████████| 203/203 [00:12<00:00, 16.76it/s]\n",
      "Epoch 20 Iter 203: loss 0.35012. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.6930395730101\n",
      "Loss: 0.31161846891964917\n"
     ]
    }
   ],
   "source": [
    "# CNN without regularization\n",
    "print('Learning rate: ', learning_rate)\n",
    "print('Batch size: ', batch_size)\n",
    "modelCNN_no_reg = CNN()\n",
    "modelCNN_no_reg = modelCNN_no_reg.to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "lr = learning_rate\n",
    "optimizer = torch.optim.Adam(modelCNN_no_reg.parameters(), lr=lr)\n",
    "model_trainer_CNN_no_reg = CNN_Trainer(modelCNN_no_reg, criterion, optimizer, 1, batch_size, lr)\n",
    "hist = model_trainer_CNN_no_reg.train(20)\n",
    "accuracy_no_reg, loss_no_reg = model_trainer_CNN_no_reg.eval_model()\n",
    "print('Accuracy:', accuracy_no_reg)\n",
    "print('Loss:', loss_no_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01160c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iter 2: loss 2.42759. :   1%|▍                                                 | 2/203 [00:00<00:15, 13.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.005\n",
      "Batch size:  362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iter 203: loss 1.06965. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 16.97it/s]\n",
      "Epoch 2 Iter 203: loss 0.82535. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.09it/s]\n",
      "Epoch 3 Iter 203: loss 0.43138. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.05it/s]\n",
      "Epoch 4 Iter 203: loss 0.67810. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.03it/s]\n",
      "Epoch 5 Iter 203: loss 0.57318. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.11it/s]\n",
      "Epoch 6 Iter 203: loss 0.52401. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.11it/s]\n",
      "Epoch 7 Iter 203: loss 0.54391. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.03it/s]\n",
      "Epoch 8 Iter 203: loss 0.60445. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.08it/s]\n",
      "Epoch 9 Iter 203: loss 0.61282. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.09it/s]\n",
      "Epoch 10 Iter 203: loss 0.43128. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.29it/s]\n",
      "Epoch 11 Iter 203: loss 0.52230. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.56it/s]\n",
      "Epoch 12 Iter 203: loss 0.35349. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.53it/s]\n",
      "Epoch 13 Iter 203: loss 0.59041. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.01it/s]\n",
      "Epoch 14 Iter 203: loss 0.34428. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.15it/s]\n",
      "Epoch 15 Iter 203: loss 0.47910. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.22it/s]\n",
      "Epoch 16 Iter 203: loss 0.54429. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.30it/s]\n",
      "Epoch 17 Iter 203: loss 0.57324. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.48it/s]\n",
      "Epoch 18 Iter 203: loss 0.36228. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.53it/s]\n",
      "Epoch 19 Iter 203: loss 0.53774. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.22it/s]\n",
      "Epoch 20 Iter 203: loss 0.59420. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.71019834282048\n",
      "Loss: 0.34462325423821977\n"
     ]
    }
   ],
   "source": [
    "# CNN with L1 regularization\n",
    "print('Learning rate: ', learning_rate)\n",
    "print('Batch size: ', batch_size)\n",
    "modelCNN_l1 = CNN()\n",
    "modelCNN_l1 = modelCNN_l1.to(device)\n",
    "# definition of parameters\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "lr = learning_rate\n",
    "optimizer = torch.optim.Adam(modelCNN_l1.parameters(), lr=lr)\n",
    "modelCNN_l1_trainer = CNN_Trainer_reg(modelCNN_l1, criterion, optimizer, 1, batch_size, lr, 'l1', 0.0001)\n",
    "hist = modelCNN_l1_trainer.train(20)\n",
    "accuracy_l1, loss_l1 = modelCNN_l1_trainer.eval_model()\n",
    "print('Accuracy:', accuracy_l1)\n",
    "print('Loss:', loss_l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63d689ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iter 2: loss 2.24003. :   1%|▍                                                 | 2/203 [00:00<00:15, 12.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.005\n",
      "Batch size:  362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iter 203: loss 0.96698. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.64it/s]\n",
      "Epoch 2 Iter 203: loss 0.37837. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.60it/s]\n",
      "Epoch 3 Iter 203: loss 0.61355. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.67it/s]\n",
      "Epoch 4 Iter 203: loss 0.37814. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.45it/s]\n",
      "Epoch 5 Iter 203: loss 0.38918. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.35it/s]\n",
      "Epoch 6 Iter 203: loss 0.33713. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.16it/s]\n",
      "Epoch 7 Iter 203: loss 0.39357. : 100%|██████████████████████████████████████████████| 203/203 [00:12<00:00, 16.86it/s]\n",
      "Epoch 8 Iter 203: loss 0.37058. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.70it/s]\n",
      "Epoch 9 Iter 203: loss 0.47387. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.28it/s]\n",
      "Epoch 10 Iter 203: loss 0.28052. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 16.92it/s]\n",
      "Epoch 11 Iter 203: loss 0.33464. : 100%|█████████████████████████████████████████████| 203/203 [00:12<00:00, 16.70it/s]\n",
      "Epoch 12 Iter 203: loss 0.39753. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.13it/s]\n",
      "Epoch 13 Iter 203: loss 0.52978. : 100%|█████████████████████████████████████████████| 203/203 [00:12<00:00, 16.88it/s]\n",
      "Epoch 14 Iter 203: loss 0.29379. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 18.04it/s]\n",
      "Epoch 15 Iter 203: loss 0.51470. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.00it/s]\n",
      "Epoch 16 Iter 203: loss 0.40248. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.39it/s]\n",
      "Epoch 17 Iter 203: loss 0.33335. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.85it/s]\n",
      "Epoch 18 Iter 203: loss 0.48155. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.96it/s]\n",
      "Epoch 19 Iter 203: loss 0.38465. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 18.09it/s]\n",
      "Epoch 20 Iter 203: loss 0.33874. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.59475544999113\n",
      "Loss: 0.31671490229956195\n"
     ]
    }
   ],
   "source": [
    "# CNN with L2 regularization with the optimizer\n",
    "print('Learning rate: ', learning_rate)\n",
    "print('Batch size: ', batch_size)\n",
    "modelCNN_l2 = CNN()\n",
    "modelCNN_l2 = modelCNN_l2.to(device)\n",
    "# definition of parameters\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "lr = learning_rate\n",
    "# To apply l2 regularization weight_decay was applied to the optimizer\n",
    "optimizer = torch.optim.Adam(modelCNN_l2.parameters(), lr=lr, weight_decay=1e-5)\n",
    "modelCNN_l2_trainer = CNN_Trainer_reg(modelCNN_l2, criterion, optimizer, 1, batch_size, lr, '')\n",
    "hist = modelCNN_l2_trainer.train(20)\n",
    "accuracy_l2, loss_l2 = modelCNN_l2_trainer.eval_model()\n",
    "print('Accuracy:', accuracy_l2)\n",
    "print('Loss:', loss_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "65807c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iter 1: loss 2.53472. :   0%|                                                          | 0/203 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.005\n",
      "Batch size:  362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iter 203: loss 1.18135. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.27it/s]\n",
      "Epoch 2 Iter 203: loss 0.63664. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.48it/s]\n",
      "Epoch 3 Iter 203: loss 0.59299. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.17it/s]\n",
      "Epoch 4 Iter 203: loss 0.59386. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.96it/s]\n",
      "Epoch 5 Iter 203: loss 0.55897. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.52it/s]\n",
      "Epoch 6 Iter 203: loss 0.66714. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.15it/s]\n",
      "Epoch 7 Iter 203: loss 0.52219. : 100%|██████████████████████████████████████████████| 203/203 [00:12<00:00, 16.80it/s]\n",
      "Epoch 8 Iter 203: loss 0.41518. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.11it/s]\n",
      "Epoch 9 Iter 203: loss 0.58663. : 100%|██████████████████████████████████████████████| 203/203 [00:11<00:00, 17.54it/s]\n",
      "Epoch 10 Iter 203: loss 0.56661. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.51it/s]\n",
      "Epoch 11 Iter 203: loss 0.51528. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.22it/s]\n",
      "Epoch 12 Iter 203: loss 0.52891. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.04it/s]\n",
      "Epoch 13 Iter 203: loss 0.45228. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 16.95it/s]\n",
      "Epoch 14 Iter 203: loss 0.47149. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.19it/s]\n",
      "Epoch 15 Iter 203: loss 0.60816. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.13it/s]\n",
      "Epoch 16 Iter 203: loss 0.49200. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.61it/s]\n",
      "Epoch 17 Iter 203: loss 0.45419. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.51it/s]\n",
      "Epoch 18 Iter 203: loss 0.42292. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.97it/s]\n",
      "Epoch 19 Iter 203: loss 0.47001. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.54it/s]\n",
      "Epoch 20 Iter 203: loss 0.46047. : 100%|█████████████████████████████████████████████| 203/203 [00:11<00:00, 17.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.54912158565051\n",
      "Loss: 0.3534956441559168\n"
     ]
    }
   ],
   "source": [
    "# CNN with Elastic regularization (L2 in the optimizer L1 applied after loss function)\n",
    "print('Learning rate: ', learning_rate)\n",
    "print('Batch size: ', batch_size)\n",
    "modelCNN_elastic = CNN()\n",
    "modelCNN_elastic = modelCNN_elastic.to(device)\n",
    "# definition of parameters\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "lr = learning_rate\n",
    "optimizer = torch.optim.Adam(modelCNN_elastic.parameters(), lr=lr, weight_decay=1e-5)\n",
    "modelCNN_elastic_trainer = CNN_Trainer_reg(modelCNN_elastic, criterion, optimizer, 1, batch_size, lr, 'l1', 0.0001)\n",
    "hist = modelCNN_elastic_trainer.train(20)\n",
    "accuracy_elastic, loss_elastic = modelCNN_elastic_trainer.eval_model()\n",
    "print('Accuracy:', accuracy_elastic)\n",
    "print('Loss:', loss_elastic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffd56bf",
   "metadata": {},
   "source": [
    "The best accuracy was when we applied L2 regularization.\n",
    "No regularization gave a better result than L1 and Elastic regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5f018c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
